{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0727f38",
   "metadata": {},
   "source": [
    "## Uczenie Nienadzorowane - projekt\n",
    "- Sebastian Borukao (259188)\n",
    "- Jakub Bukaa (259189)\n",
    "- Jakub Smakowski (259209)\n",
    "\n",
    "Celem projektu jest zbudowanie modelu uzupeniajcego (inpainting) uszkodzenia dzie sztuki pochodzcyzh z platformy WikiArt w oparciu o metody uczenia nienadzorowanego (unsupervised) i samonadzorowanego (self-supervised) nale偶y zbudowa system pozwalajcy na grupowanie obraz贸w na podstawie ich podobiestwa oraz uzupenianie ich symulowanych uszkodze.\n",
    "\n",
    "1. Budow systemu nale偶y rozpocz od budowy generatora uszkodze obraz贸w (biaych plam /masek). Podstawowa wersja uszkadzania obraz贸w polega na losowym wycinaniu / maskowaniu kwadratowych fragment贸w obrazu, zajmujcych nie wicej ni偶 1/16 obrazu. Rozszerzona wersja powinna pozwala na generowanie uszkodze o nieregularnych ksztatach.\n",
    "2. Kolejnym krokiem powinno by zbudowanie zredukowanej reprezentacji obraz贸w ze zbioru treningowego a nastpnie jej klasteryzacji w celu znalezienia naturalnego grupowania styli. Grupowanie nale偶y nastpnie por贸wna z dostpnymi w zbiorze etykietami stylu i autora.\n",
    "3. Nastpnie, dla ka偶dej z grup nale偶y nauczy osobny model uzupeniajcy uszkodzenia. Alternatywnie, mo偶liwe jest zbudowanie jednego modelu kt贸ry na wejciu przyjmowaby uszkodzony obraz oraz identyfikator grupy.\n",
    "4. Kocowym etapem bdzie zbudowanie modelu zwikszajcego rozdzielczo obraz贸w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72394b2c",
   "metadata": {},
   "source": [
    "# ETAP 1\n",
    "..................."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e529681e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f90571ae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc2ff83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3477c05",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74410f03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b71b6683",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4d06764",
   "metadata": {},
   "source": [
    "# ETAP KOCOWY: Super-Resolution  \n",
    "Celem tego kroku projektu jest stworzenie modelu Super-Resolution (SR), kt贸ry potrafi zwikszy rozdzielczo obraz贸w dzie sztuki (zbi贸r WikiArt) z zachowaniem detali malarskich (pocignicia pdzla, faktura p贸tna).\n",
    "\n",
    "**Wykorzystane technologie:**  \n",
    "- Model: SRCNN (Super-Resolution Convolutional Neural Network).  \n",
    "- Dane: WikiArt (Obrazy artystyczne).  \n",
    "- rodowisko: Kaggle (GPU P100/T4).  \n",
    "- ledzenie eksperyment贸w: Comet ML.  \n",
    "\n",
    "**Opis modelu:**\n",
    "SRCNN realizuje proces w trzech krokach:\n",
    "1. **Ekstrakcja:** Warstwa 1 wyodrbnia cechy z obrazu LR.\n",
    "2. **Mapowanie:** Warstwa 2 mapuje cechy na przestrze wysokiej rozdzielczoci.\n",
    "3. **Rekonstrukcja:** Warstwa 3 skada obraz kocowy HR.\n",
    "\n",
    "**Stosowane metryki:**\n",
    "* **PSNR (dB):** Podstawowa miara jakoci rekonstrukcji obrazu (wyra偶ana w dB). Im wy偶sza, tym lepiej.\n",
    "* **L1 Loss:** Funkcja straty u偶ywana podczas treningu. W przeciwiestwie do MSE, L1 rzadziej powoduje nadmierne \"rozmycie\" obrazu, co jest kluczowe przy dzieach sztuki.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b970ca2",
   "metadata": {},
   "source": [
    "###  Krok 1: Pobranie Zbioru Danych (HR)  \n",
    "Wykorzystujemy bibliotek datasets z Hugging Face do pobrania zbioru HR (huggan/wikiart)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c0aeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from tqdm import tqdm # Dodajemy tqdm do ledzenia postpu w ptli\n",
    "\n",
    "DATASET_NAME = \"huggan/wikiart\"\n",
    "\n",
    "# U偶ywamy cie偶ek bezwzgldnych.\n",
    "SAVE_DIR = r\"C:\\Users\\admin\\Documents\\SIIUM2\\UN\\wikiart_hr_images\"\n",
    "SPLIT_DIR = r\"C:\\Users\\admin\\Documents\\SIIUM2\\UN\\data_splits\"\n",
    "\n",
    "# Tworzymy katalogi, jeli nie istniej\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(SPLIT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"adowanie penego zbioru danych: {DATASET_NAME}...\")\n",
    "\n",
    "try:\n",
    "    # Wczytujemy cay zbi贸r danych (split=\"train\")\n",
    "    dataset = load_dataset(DATASET_NAME, split=\"train\", cache_dir=\"./hf_cache\")\n",
    "    print(f\"Zbi贸r danych zaadowany. Cakowita liczba obraz贸w: {len(dataset)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Bd adowania zbioru danych: {e}\")\n",
    "    exit()\n",
    "\n",
    "image_paths = []\n",
    "total_images = len(dataset)\n",
    "\n",
    "print(f\"Rozpoczynanie zapisu {total_images} obraz贸w na dysk przy u偶yciu ptli for...\")\n",
    "\n",
    "# Zapis Obraz贸w na Dysk\n",
    "for index, example in enumerate(tqdm(dataset, total=total_images, desc=\"Zapisywanie obraz贸w HR\")):\n",
    "    try:\n",
    "        img = example.get('image')\n",
    "        \n",
    "        if img is None:\n",
    "            continue\n",
    "            \n",
    "        filename = os.path.join(SAVE_DIR, f\"{index:06d}.png\")\n",
    "        \n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "            \n",
    "        img.save(filename)\n",
    "        image_paths.append(filename) # Dodajemy cie偶k do listy\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nOstrze偶enie: Bd zapisu obrazu {index}: {e}. Plik pominity.\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nZapis zakoczony. Liczba poprawnie zapisanych plik贸w: {len(image_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e33d714",
   "metadata": {},
   "source": [
    "###  Krok 2: Przetworzenie oryginalnych obraz贸w o wysokiej rozdzielczoci do rozmiaru 512x512 i podzia na zbiory: treningowy, walidacyjny i testowy (pliki JSON)  \n",
    "Zastosowalimy Center Crop, aby unikn znieksztace proporcji przy skalowaniu do kwadratu 512x512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078ef434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import multiprocessing\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- KONFIGURACJA ---\n",
    "SOURCE_DIR = r\"C:\\Users\\admin\\Documents\\SIIUM2\\4_UN\\wikiart_hr_images\" # 殴r贸do (du偶e pliki)\n",
    "TARGET_DIR = r\"C:\\Users\\admin\\Documents\\SIIUM2\\4_UN\\wikiart_hr_512\" # Cel (mae pliki 512x512)\n",
    "SPLIT_DIR = r\"C:\\Users\\admin\\Documents\\SIIUM2\\4_UN\\data_splits_512\" # Folder na JSONy\n",
    "\n",
    "TARGET_SIZE = 512\n",
    "TEST_SIZE = 0.15\n",
    "VALIDATION_SIZE = 0.15 \n",
    "# --------------------\n",
    "\n",
    "os.makedirs(TARGET_DIR, exist_ok=True)\n",
    "os.makedirs(SPLIT_DIR, exist_ok=True)\n",
    "\n",
    "def process_image(filename):\n",
    "    \"\"\"Skaluje i przycina obraz, zwraca nazw pliku jeli sukces, None jeli bd\"\"\"\n",
    "    src_path = os.path.join(SOURCE_DIR, filename)\n",
    "    dst_path = os.path.join(TARGET_DIR, filename)\n",
    "    \n",
    "    # Optymalizacja: jeli plik ju偶 jest, nie robimy go drugi raz, ale zwracamy nazw\n",
    "    if os.path.exists(dst_path):\n",
    "        return filename\n",
    "\n",
    "    try:\n",
    "        with Image.open(src_path) as img:\n",
    "            img = img.convert('RGB')\n",
    "            w, h = img.size\n",
    "            \n",
    "            # Odrzucamy zbyt mae obrazy\n",
    "            if w < TARGET_SIZE or h < TARGET_SIZE:\n",
    "                return None\n",
    "            \n",
    "            # 1. Resize (skalowanie kr贸tszego boku do 512)\n",
    "            scale = TARGET_SIZE / min(w, h)\n",
    "            new_w = int(w * scale)\n",
    "            new_h = int(h * scale)\n",
    "            img = img.resize((new_w, new_h), resample=Image.Resampling.LANCZOS)\n",
    "            \n",
    "            # 2. Center Crop (wycicie rodka)\n",
    "            left = (new_w - TARGET_SIZE) // 2\n",
    "            top = (new_h - TARGET_SIZE) // 2\n",
    "            right = left + TARGET_SIZE\n",
    "            bottom = top + TARGET_SIZE\n",
    "            img = img.crop((left, top, right, bottom))\n",
    "            \n",
    "            img.save(dst_path, format='PNG')\n",
    "            return filename\n",
    "            \n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 1. Pobranie listy plik贸w 藕r贸dowych\n",
    "    print(\"Skanowanie katalogu 藕r贸dowego...\")\n",
    "    all_files = [f for f in os.listdir(SOURCE_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    print(f\"Znaleziono {len(all_files)} plik贸w. Start przetwarzania...\")\n",
    "    \n",
    "    successful_files = []\n",
    "\n",
    "    # 2. Przetwarzanie r贸wnolege\n",
    "    num_workers = max(1, multiprocessing.cpu_count() - 2)\n",
    "    \n",
    "    with multiprocessing.Pool(num_workers) as pool:\n",
    "        # imap_unordered zwraca wyniki w miar ich koczenia\n",
    "        for result in tqdm(pool.imap_unordered(process_image, all_files), total=len(all_files)):\n",
    "            if result is not None:\n",
    "                successful_files.append(result)\n",
    "\n",
    "    print(f\"\\nPrzetworzono poprawnie: {len(successful_files)} obraz贸w.\")\n",
    "\n",
    "    # 3. Tworzenie podziau na zbiory (Train/Val/Test)\n",
    "    print(\"Tworzenie plik贸w JSON z podziaem na zbiory...\")\n",
    "    \n",
    "    # Pene cie偶ki do nowych plik贸w\n",
    "    full_paths = [os.path.join(TARGET_DIR, f) for f in successful_files]\n",
    "\n",
    "    # Podzia: Najpierw wydzielamy Test\n",
    "    train_val_paths, test_paths = train_test_split(\n",
    "        full_paths, test_size=TEST_SIZE, random_state=42\n",
    "    )\n",
    "\n",
    "    # Podzia: Reszt dzielimy na Train i Validation\n",
    "    # Przeliczamy proporcj, bo pula si zmniejszya o test\n",
    "    val_split_ratio = VALIDATION_SIZE / (1.0 - TEST_SIZE)\n",
    "    train_paths, val_paths = train_test_split(\n",
    "        train_val_paths, test_size=val_split_ratio, random_state=42\n",
    "    )\n",
    "\n",
    "    splits = {\n",
    "        \"train\": train_paths,\n",
    "        \"validation\": val_paths,\n",
    "        \"test\": test_paths\n",
    "    }\n",
    "\n",
    "    for split_name, paths_list in splits.items():\n",
    "        file_path = os.path.join(SPLIT_DIR, f\"{split_name}_paths.json\")\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(paths_list, f)\n",
    "        print(f\" -> {split_name}: {len(paths_list)} obraz贸w zapisano w {file_path}\")\n",
    "\n",
    "    print(\"\\nGOTOWE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f932b2",
   "metadata": {},
   "source": [
    "###  Krok 3: G贸wny Trening na Kaggle (krok3 - kaggle notebook)\n",
    "Cel: Serce projektu. Tutaj definiujemy model SRCNN, adujemy dane, trenujemy sie i logujemy wyniki do Comet ML.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3e7816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfiguracja COMET\n",
    "!pip install comet_ml\n",
    "\n",
    "import comet_ml\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# Pobranie klucza z sekret贸w Kaggle\n",
    "user_secrets = UserSecretsClient()\n",
    "COMET_API_KEY = user_secrets.get_secret(\"COMET_API_KEY\")\n",
    "\n",
    "import os\n",
    "from datetime import datetime, timezone, timedelta\n",
    "current_time = (datetime.now(timezone.utc) + timedelta(hours=1)).strftime(\"%Y_%m_%d_%H%M\")\n",
    "unique_name = f\"SR_{current_time}\"\n",
    "\n",
    "# Inicjalizacja eksperymentu\n",
    "experiment = comet_ml.Experiment(\n",
    "    api_key=COMET_API_KEY,\n",
    "    project_name=\"wikiart-super-resolution\",\n",
    "    workspace=\"Smaku\",\n",
    "    auto_metric_logging=True\n",
    ")\n",
    "\n",
    "experiment.set_name(unique_name)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm # tqdm wersja dla notebook贸w\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.cuda.amp import GradScaler, autocast \n",
    "\n",
    "#cie偶ka do zdj\n",
    "IMG_DIR = \"/kaggle/input/huggan-wikiart-512x512/wikiart_hr_512\" \n",
    "# cie偶ki do JSON贸w\n",
    "TRAIN_JSON = \"/kaggle/input/huggan-wikiart-512x512/data_splits_512/train_paths.json\"\n",
    "VAL_JSON = \"/kaggle/input/huggan-wikiart-512x512/data_splits_512/validation_paths.json\"\n",
    "\n",
    "# --- KLASA DATASET ---\n",
    "# Obsuguje konwersj cie偶ek Windowsowych do Kaggle'owych oraz degradacj w locie (LR z HR) przy u偶yciu Resize - BICUBIC\n",
    "class WikiArtKaggleDataset(Dataset):\n",
    "    def __init__(self, json_path, img_dir):\n",
    "        with open(json_path, 'r') as f:\n",
    "            self.raw_paths = json.load(f)\n",
    "        \n",
    "        self.img_dir = img_dir\n",
    "        \n",
    "        self.hr_transform = T.Compose([T.ToTensor()])\n",
    "        # Degradacja w locie\n",
    "        self.lr_transform = T.Compose([\n",
    "            T.Resize((256, 256), interpolation=T.InterpolationMode.BICUBIC),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. Bierzemy star cie偶k z Windowsa\n",
    "        windows_path = self.raw_paths[idx]\n",
    "        \n",
    "        # 2. Wycigamy sam nazw pliku (np. \"00123.png\")\n",
    "        # U偶ywamy ntpath, 偶eby obsu偶y slashe z Windowsa na Linuxie\n",
    "        import ntpath\n",
    "        filename = ntpath.basename(windows_path)\n",
    "        \n",
    "        # 3. Budujemy now, poprawn cie偶k na Kaggle\n",
    "        full_path = os.path.join(self.img_dir, filename)\n",
    "        \n",
    "        try:\n",
    "            img = Image.open(full_path).convert('RGB')\n",
    "            return self.lr_transform(img), self.hr_transform(img)\n",
    "        except Exception as e:\n",
    "            # Fallback w razie bdu\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "# --- MODEL SRCNN ---        \n",
    "class SRCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SRCNN, self).__init__()\n",
    "        # Etap 0: Powikszenie wstpne (Bicubic) do rozmiaru docelowego\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bicubic', align_corners=False)\n",
    "        # Etap 1: Ekstrakcja cech (Patch extraction and representation)\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=9, padding=4)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        # Etap 2: Mapowanie nieliniowe (Non-linear mapping)\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=5, padding=2)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        # Etap 3: Rekonstrukcja (Reconstruction)\n",
    "        self.conv3 = nn.Conv2d(32, 3, kernel_size=5, padding=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.upsample(x) \n",
    "        out = self.relu1(self.conv1(x))\n",
    "        out = self.relu2(self.conv2(out))\n",
    "        out = self.conv3(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "# Ustawienia\n",
    "BATCH_SIZE = 64 \n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 40\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Katalog wyjciowy (na Kaggle zapisujemy tylko do /kaggle/working/)\n",
    "SAVE_DIR = \"/kaggle/working/results\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Dataset i DataLoader\n",
    "train_dataset = WikiArtKaggleDataset(TRAIN_JSON, IMG_DIR)\n",
    "val_dataset = WikiArtKaggleDataset(VAL_JSON, IMG_DIR)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4,pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4,pin_memory=True)\n",
    "\n",
    "# ... (inicjalizacja modelu) ...\n",
    "model = SRCNN().to(DEVICE)\n",
    "# criterion = nn.MSELoss() # Mean Squared Error\n",
    "criterion = nn.L1Loss() # Mean Absolute Error\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scaler = torch.amp.GradScaler('cuda') # <--- (float32 -> float16)\n",
    "\n",
    "# Scheduler: Zmniejszy LR o poow (factor=0.5), jeli przez 3 epoki (patience=3) warto val_loss nie spadnie.\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min',       # Reagujemy, gdy val_loss przestaje SPADA\n",
    "    factor=0.5,       # Mno偶ymy LR * 0.5 (zmniejszamy o poow)\n",
    "    patience=3,       # Czekamy 3 epoki zanim zareagujemy\n",
    "    min_lr=1e-6       # Nie schodzimy poni偶ej tej wartoci\n",
    ")\n",
    "\n",
    "def calculate_psnr(img1, img2, border=0):\n",
    "    if border > 0:\n",
    "        img1 = img1[..., border:-border, border:-border]\n",
    "        img2 = img2[..., border:-border, border:-border]\n",
    "        \n",
    "    mse = torch.mean((img1 - img2) ** 2)\n",
    "    if mse == 0: return 100\n",
    "    return 10 * torch.log10(1. / mse)\n",
    "\n",
    "# Logowanie parametr贸w do Cometa\n",
    "experiment.log_parameters({\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"lr\": LEARNING_RATE,\n",
    "    \"epochs\": NUM_EPOCHS,\n",
    "    \"model\": \"SRCNN\"\n",
    "})\n",
    "\n",
    "# --- WZNAWIANIE TRENINGU ---\n",
    "START_EPOCH = 0\n",
    "RESUME_PATH = \"/kaggle/working/results/srcnn_epoch_10.pth\" # cie偶ka do ostatniego zapisu\n",
    "\n",
    "if os.path.exists(RESUME_PATH):\n",
    "    print(f\"Wczytuj wagi z: {RESUME_PATH}\")\n",
    "    model.load_state_dict(torch.load(RESUME_PATH))\n",
    "    START_EPOCH = 10 # Ustawiamy, 偶e zaczynamy od kolejnej epoki\n",
    "else:\n",
    "    print(\"Brak zapisu, zaczynam trening od zera.\")\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(START_EPOCH,NUM_EPOCHS):\n",
    "    model.train()\n",
    "\n",
    "    # 1. Zmienna na sumowanie straty w tej epoce\n",
    "    running_train_loss = 0.0\n",
    "    \n",
    "    loop = tqdm(train_loader, desc=f\"Epoka {epoch+1}\")\n",
    "    \n",
    "    for lr_imgs, hr_imgs in loop:\n",
    "        lr_imgs, hr_imgs = lr_imgs.to(DEVICE), hr_imgs.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # --- Obliczenia w float16 ---\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            outputs = model(lr_imgs)\n",
    "            loss = criterion(outputs, hr_imgs)\n",
    "        \n",
    "        # 2. Dodajemy strat z tego batcha do sumy\n",
    "        running_train_loss += loss.item()\n",
    "        \n",
    "        # --- Skalowanie gradient贸w ---\n",
    "        # Zamiast loss.backward() i optimizer.step() u偶ywamy scalera\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        experiment.log_metric(\"train_loss_step\", loss.item(), step=global_step, epoch=epoch+1)\n",
    "        global_step += 1\n",
    "\n",
    "    # 3. Obliczamy redni\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "    \n",
    "    # Walidacja\n",
    "    model.eval()\n",
    "    val_psnr = 0\n",
    "    val_loss = 0  \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for lr_imgs, hr_imgs in val_loader:\n",
    "            lr_imgs, hr_imgs = lr_imgs.to(DEVICE), hr_imgs.to(DEVICE)\n",
    "            \n",
    "            # 1. Generujemy obraz\n",
    "            outputs = model(lr_imgs)\n",
    "            # 2. Liczymy strat (Loss) dla walidacji\n",
    "            loss = criterion(outputs, hr_imgs)\n",
    "            val_loss += loss.item() # <--- Sumujemy strat\n",
    "            # 3. Liczymy PSNR\n",
    "            val_psnr += calculate_psnr(outputs, hr_imgs, border=4).item()\n",
    "    \n",
    "    # Obliczamy rednie\n",
    "    avg_val_loss = val_loss / len(val_loader) \n",
    "    avg_val_psnr = val_psnr / len(val_loader)\n",
    "    \n",
    "    print(f\"Epoka {epoch+1} zakoczona.\")\n",
    "    print(f\" -> Train Loss (avg): {avg_train_loss:.6f}\")\n",
    "    print(f\" -> Val Loss: {avg_val_loss:.6f}\")\n",
    "    print(f\" -> Val PSNR: {avg_val_psnr:.2f} dB\")\n",
    "    \n",
    "    # Logowanie metryki epoki do Cometa\n",
    "    experiment.log_metric(\"train_loss_epoch\", avg_train_loss, step=global_step, epoch=epoch+1)\n",
    "    experiment.log_metric(\"val_psnr_epoch\", avg_val_psnr, step=global_step, epoch=epoch+1)\n",
    "    experiment.log_metric(\"val_loss_epoch\", avg_val_loss, step=global_step, epoch=epoch+1)\n",
    "\n",
    "    # Scheduler:\n",
    "    # 1. Karmimy scheduler wynikiem walidacji (musi wiedzie, czy jest postp)\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    # 2. Pobieramy aktualny Learning Rate, 偶eby wysa go do Cometa\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    experiment.log_metric(\"learning_rate\", current_lr, step=global_step, epoch=epoch+1)\n",
    "        \n",
    "    # Zapis na dysku Kaggle\n",
    "    model_path = f\"{SAVE_DIR}/srcnn_epoch_{epoch+1}.pth\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    # WYSYKA DO CHMURY COMET (Bezpieczna kopia)\n",
    "    experiment.log_model(f\"SRCNN_Epoch_{epoch+1}\", model_path)\n",
    "    \n",
    "    # Wysyanie przykadowego obrazu do Cometa\n",
    "    # Wizualizacja por贸wnawcza\n",
    "    with torch.no_grad():\n",
    "        # Bierzemy pierwszy obraz z batcha walidacyjnego (ostatni batch z ptli walidacji)\n",
    "        \n",
    "        # 1. Przygotowanie SR (Super Resolution - wynik modelu)\n",
    "        sr_img = outputs[0].cpu().permute(1, 2, 0).numpy() # Z (C,H,W) na (H,W,C)\n",
    "        sr_img = np.clip(sr_img, 0, 1) # Zabezpieczenie zakresu\n",
    "\n",
    "        # 2. Przygotowanie HR (High Resolution - cel)\n",
    "        hr_img = hr_imgs[0].cpu().permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # 3. Przygotowanie LR (Low Resolution - wejcie)\n",
    "        lr_img_tensor = lr_imgs[0].cpu()\n",
    "        # Resize, aby pasowao wymiarem do paska (LR jest mniejsze, trzeba je rozcign do podgldu)\n",
    "        lr_img_resized = T.Resize((hr_img.shape[0], hr_img.shape[1]), interpolation=T.InterpolationMode.BICUBIC)(lr_img_tensor)\n",
    "        \n",
    "        lr_img = lr_img_resized.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # 4. Sklejenie obraz贸w horyzontalnie: [Wejcie] | [Model] | [Orygina]\n",
    "        combined_img = np.hstack((lr_img, sr_img, hr_img))\n",
    "        \n",
    "        # 5. Logowanie do Cometa\n",
    "        experiment.log_image(combined_img, name=f\"Porownanie_Epoka_{epoch+1}\", step=global_step)\n",
    "\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3991d5be",
   "metadata": {},
   "source": [
    "###  KROK 4: Wykorzystanie wytrenowanego modelu do zwikszenia rozdzielczoci obrazu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ac2252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- KLASA SRCNN ---\n",
    "class SRCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SRCNN, self).__init__()\n",
    "        # Etap 0: Powikszenie wstpne (Bicubic) do rozmiaru docelowego\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bicubic', align_corners=False)\n",
    "\n",
    "        # Etap 1: Ekstrakcja cech (Patch extraction and representation)\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=9, padding=4)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Etap 2: Mapowanie nieliniowe (Non-linear mapping)\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=5, padding=2)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Etap 3: Rekonstrukcja (Reconstruction)\n",
    "        self.conv3 = nn.Conv2d(32, 3, kernel_size=5, padding=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.upsample(x) \n",
    "        out = self.relu1(self.conv1(x))\n",
    "        out = self.relu2(self.conv2(out))\n",
    "        out = self.conv3(out)\n",
    "        return out\n",
    "\n",
    "# --- FUNKCJA UPSCALINGU x4 W DWCH KROKACH ---\n",
    "def upscale_x4_progressive(image_path, model_path, output_name=\"result_1024x1024.png\"):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 1. adowanie modelu\n",
    "    print(f\"adowanie modelu z: {model_path}\")\n",
    "    model = SRCNN().to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # 2. Wczytanie obrazu (256x256)\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    transform = T.ToTensor()\n",
    "    # input_tensor: [1, 3, 256, 256]\n",
    "    input_tensor = transform(img).unsqueeze(0).to(device)\n",
    "    \n",
    "    print(f\"Start: {img.size}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # --- KROK 1: 256 -> 512 ---\n",
    "        print(\"Krok 1: Upscaling x2 (do 512x512)...\")\n",
    "        stage1_output = model(input_tensor)\n",
    "        \n",
    "        # Wa偶ne: przycinamy wartoci do 0-1 przed drugim krokiem, \n",
    "        # 偶eby model w drugim przebiegu dosta poprawne dane \"obrazowe\"\n",
    "        stage1_output = stage1_output.clamp(0, 1)\n",
    "\n",
    "        # --- KROK 2: 512 -> 1024 ---\n",
    "        # Wynik kroku 1 wchodzi jako wejcie do kroku 2!\n",
    "        print(\"Krok 2: Upscaling x2 (do 1024x1024)...\")\n",
    "        final_output = model(stage1_output)\n",
    "        \n",
    "        final_output = final_output.clamp(0, 1)\n",
    "\n",
    "    # 3. Zapis i wizualizacja\n",
    "    result_img = T.ToPILImage()(final_output.squeeze(0).cpu())\n",
    "    result_img.save(output_name)\n",
    "    print(f\"Gotowe! Wynikowy rozmiar: {result_img.size}\")\n",
    "    print(f\"Zapisano jako: {output_name}\")\n",
    "\n",
    "    # Por贸wnanie\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Input ({img.size})\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(result_img)\n",
    "    plt.title(f\"SRCNN x4 Output ({result_img.size})\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# --- U呕YCIE ---\n",
    "upscale_x4_progressive(\"042222-256x256.png\", \"srcnn_epoch_31.pth\", output_name=\"result_1024x1024.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea38847",
   "metadata": {},
   "source": [
    "Podsumowanie: Model SRCNN wykazuje znaczn popraw ostroci w por贸wnaniu do standardowej interpolacji bicubic."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
