{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Shortened setup - copy of previous work (dowload all the data, sort and split)"
      ],
      "metadata": {
        "id": "FGfu_JMQoah_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAlL2kirnAZ3"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightning"
      ],
      "metadata": {
        "id": "qX7hqPlzoX9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install comet_ml"
      ],
      "metadata": {
        "id": "S7viNnHPpjcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up Google Drive Access\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Q1N5vRR8nfVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle"
      ],
      "metadata": {
        "id": "7GItzhpLnfXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -halt /content/drive/MyDrive/DLF_2025/"
      ],
      "metadata": {
        "id": "1e-NlHrwnfaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/DLF_2025/kaggle.json ~/.kaggle"
      ],
      "metadata": {
        "id": "VdOWZi5jnfcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "9_mJldxrnfgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test kaggle\n",
        "!kaggle datasets list"
      ],
      "metadata": {
        "id": "40Xemnc6nfi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we will now move the .comet.config file to correct directory\n",
        "# to create this file follow the tutorial: https://www.comet.com/docs/v2/guides/experiment-management/configure-sdk/\n",
        "\n",
        "!cp /content/drive/MyDrive/DLF_2025/.comet.config ~/.comet.config"
      ],
      "metadata": {
        "id": "TjaFnfItp5u8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test if CometML is installed correctly\n",
        "!comet login\n",
        "# if the correct file is in correct place this should output nothing, otherwise it will ask for your comet API key\n"
      ],
      "metadata": {
        "id": "bOBwKyGHqQdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading the datasets"
      ],
      "metadata": {
        "id": "sBBWhXWfnlUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -p ./datasets/waste-classification-data --unzip techsash/waste-classification-data"
      ],
      "metadata": {
        "id": "9XsdPFRgnflK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -p ./datasets/garbage-classification --unzip asdasdasasdas/garbage-classification"
      ],
      "metadata": {
        "id": "YHvSIIAHnfnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -p ./datasets/recyclable-and-household-waste-classification --unzip alistairking/recyclable-and-household-waste-classification"
      ],
      "metadata": {
        "id": "Hsh8cdFLnpmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -p ./datasets/garbage-classification-12-classes --unzip mostafaabla/garbage-classification\n"
      ],
      "metadata": {
        "id": "2PeGqRyhnpoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -p ./datasets/garbage-classification-v2 --unzip sumn2u/garbage-classification-v2\n"
      ],
      "metadata": {
        "id": "svCAFxylnpq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -p ./datasets/garbage-dataset-classification --unzip zlatan599/garbage-dataset-classification\n"
      ],
      "metadata": {
        "id": "I16vJKKInptK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -p ./datasets/plastic-paper-garbage-bag-synthetic-images --unzip vencerlanz09/plastic-paper-garbage-bag-synthetic-images\n"
      ],
      "metadata": {
        "id": "4xrr_4hDnpvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd datasets/garbage-classification  && rm -rf \"Garbage classification\""
      ],
      "metadata": {
        "id": "EgUt9X1vnpx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd datasets && mv \"garbage-classification/garbage classification\"/\"Garbage classification\" garbage_classification"
      ],
      "metadata": {
        "id": "LIFBMvvcnpz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd datasets && rm -rf \"garbage-classification\""
      ],
      "metadata": {
        "id": "EhDvACOgnwta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd datasets/waste-classification-data/ && mv DATASET ./waste_classification_data"
      ],
      "metadata": {
        "id": "DepBjQ1enwvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd datasets/waste-classification-data/ && rm -rf dataset &&"
      ],
      "metadata": {
        "id": "DLdBzAvqnwyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd datasets/waste-classification-data/ && mv waste_classification_data ../"
      ],
      "metadata": {
        "id": "MC6CVJz7nw0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd datasets && rm -rf waste-classification-data"
      ],
      "metadata": {
        "id": "d1q4nWFYnw2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# new additions to the dataset\n",
        "# https://www.kaggle.com/datasets/alistairking/recyclable-and-household-waste-classification\n",
        "# https://www.kaggle.com/datasets/mostafaabla/garbage-classification\n",
        "# https://www.kaggle.com/datasets/sumn2u/garbage-classification-v2\n",
        "# https://www.kaggle.com/datasets/zlatan599/garbage-dataset-classification\n",
        "# https://www.kaggle.com/datasets/vencerlanz09/plastic-paper-garbage-bag-synthetic-images\n",
        "\n",
        "# Ones that are already downloaded (previous scripts), but not combined fully yet\n",
        "# https://www.kaggle.com/datasets/techsash/waste-classification-data\n",
        "# https://www.kaggle.com/datasets/asdasdasasdas/garbage-classification\n",
        "\n",
        "\n",
        "from os import listdir, makedirs\n",
        "import os\n",
        "from os.path import join\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "final_classes = {\n",
        "    'organic': 0,\n",
        "    'battery': 1,\n",
        "    'glass': 2,\n",
        "    'metal': 3,\n",
        "    'paper': 4,\n",
        "    'cardboard': 5,\n",
        "    'plastic': 6,\n",
        "    'textiles': 7,\n",
        "    'trash': 8,\n",
        "}\n",
        "\n",
        "mappings = {\n",
        "    'recyclable-and-household-waste-classification': {\n",
        "        'aerosol_cans': 'metal',\n",
        "        'aluminum_food_cans': 'metal',\n",
        "        'aluminum_soda_cans': 'metal',\n",
        "        'cardboard_boxes': 'cardboard',\n",
        "        'cardboard_packaging': 'cardboard',\n",
        "        'clothing': 'textiles',\n",
        "        'coffee_grounds': 'organic',\n",
        "        'disposable_plastic_cutlery': 'plastic',\n",
        "        'eggshells': 'organic',\n",
        "        'food_waste': 'organic',\n",
        "        'glass_beverage_bottles': 'glass',\n",
        "        'glass_cosmetic_containers': 'glass',\n",
        "        'glass_food_jars': 'glass',\n",
        "        'magazines': 'paper',\n",
        "        'newspaper': 'paper',\n",
        "        'office_paper': 'paper',\n",
        "        'paper_cups': 'paper',\n",
        "        'plastic_cup_lids': 'plastic',\n",
        "        'plastic_detergent_bottles': 'plastic',\n",
        "        'plastic_food_containers': 'plastic',\n",
        "        'plastic_shopping_bags': 'plastic',\n",
        "        'plastic_soda_bottles': 'plastic',\n",
        "        'plastic_straws': 'plastic',\n",
        "        'plastic_trash_bags': 'plastic',\n",
        "        'plastic_water_bottles': 'plastic',\n",
        "        'shoes': 'textiles',\n",
        "        'steel_food_cans': 'metal',\n",
        "        'styrofoam_cups': 'plastic',\n",
        "        'styrofoam_food_containers': 'plastic',\n",
        "        'tea_bags': 'organic',\n",
        "    },\n",
        "    'waste-classification-data': {\n",
        "        'O': 'organic',\n",
        "    },\n",
        "    'plastic-paper-garbage-bag-synthetic-images': {\n",
        "        'Garbage Bag Images': 'plastic',\n",
        "        'Paper Bag Images': 'paper',\n",
        "        'Plastic Bag Images': 'plastic'\n",
        "    },\n",
        "    'garbage-dataset-classification': {\n",
        "        'cardboard': 'cardboard',\n",
        "        'glass': 'glass',\n",
        "        'metal': 'metal',\n",
        "        'paper': 'paper',\n",
        "        'plastic': 'plastic',\n",
        "        'trash': 'trash',\n",
        "    },\n",
        "    'garbage-classification-v2': {\n",
        "        'battery': 'battery',\n",
        "        'biological': 'organic',\n",
        "        'cardboard': 'cardboard',\n",
        "        'clothes': 'textiles',\n",
        "        'glass': 'glass',\n",
        "        'metal': 'metal',\n",
        "        'paper': 'paper',\n",
        "        'plastic': 'plastic',\n",
        "        'shoes': 'textiles',\n",
        "        'trash': 'trash',\n",
        "    },\n",
        "    'garbage-classification-12-classes': {\n",
        "        'battery': 'battery',\n",
        "        'biological': 'organic',\n",
        "        'brown-glass': 'glass',\n",
        "        'cardboard': 'cardboard',\n",
        "        'clothes': 'textiles',\n",
        "        'green-glass': 'glass',\n",
        "        'metal': 'metal',\n",
        "        'paper': 'paper',\n",
        "        'plastic': 'plastic',\n",
        "        'shoes': 'textiles',\n",
        "        'trash': 'trash',\n",
        "        'white-glass': 'glass',\n",
        "    },\n",
        "    'garbage_classification': {\n",
        "        'cardboard': 'cardboard',\n",
        "        'glass': 'glass',\n",
        "        'metal': 'metal',\n",
        "        'paper': 'paper',\n",
        "        'plastic': 'plastic',\n",
        "        'trash': 'trash',\n",
        "    },\n",
        "}\n",
        "\n",
        "output_root = './datasets/combined_waste_dataset'\n",
        "\n",
        "# step 1 - create directory structure for the combined dataset\n",
        "# params - root_dir (output directory, main path), classes_dict (final classes dictionary)\n",
        "# just main directories, no splitting and subdirectories\n",
        "\n",
        "def create_directory_structure(root_dir, classes_dict):\n",
        "    for class_name in classes_dict.keys():\n",
        "        class_dir = join(root_dir, class_name)\n",
        "        makedirs(class_dir, exist_ok=True)\n",
        "    print(f\"Created directory structure in {root_dir}\")\n",
        "\n",
        "\n",
        "create_directory_structure(output_root, final_classes)\n",
        "\n",
        "# step 2 - move images from each dataset to the combined directory structure\n",
        "# create a separate function for each dataset (separate processing for clarity)\n",
        "# functions are named according to the dataset they process (no \"general function\")\n",
        "\n",
        "# first is the waste_classification_data dataset\n",
        "# folder structure: two folders TRAIN and TEST, each with two subfolders O and R\n",
        "# we will ignore the split and just move all images to the combined dataset\n",
        "# moreover, we will map according to the \"mappings\" dictionary above (so ignore the R class, it is not split into recyclable types)\n",
        "\n",
        "def process_waste_classification_data(source_dir, dest_dir_root, mapping):\n",
        "    for split in ['TRAIN', 'TEST']:\n",
        "        split_dir = join(source_dir, split)\n",
        "        for class_folder in tqdm(listdir(split_dir), desc=f\"Processing {split} of waste_classification_data\"):\n",
        "            class_dir = join(split_dir, class_folder)\n",
        "            if class_folder in mapping:\n",
        "                final_class = mapping[class_folder]\n",
        "                final_class_dir = join(dest_dir_root, final_class)\n",
        "                for img_file in tqdm(listdir(class_dir), desc=f\"Processing images in {class_folder}\"):\n",
        "                    src_path = join(class_dir, img_file)\n",
        "                    dest_path = join(final_class_dir, img_file)\n",
        "                    if os.path.exists(dest_path):\n",
        "                        # If file already exists, create a unique name\n",
        "                        base, ext = os.path.splitext(img_file)\n",
        "                        counter = 1\n",
        "                        new_name = f\"{base}_{counter}{ext}\"\n",
        "                        new_dest_path = join(final_class_dir, new_name)\n",
        "                        while os.path.exists(new_dest_path):\n",
        "                            counter += 1\n",
        "                            new_name = f\"{base}_{counter}{ext}\"\n",
        "                            new_dest_path = join(final_class_dir, new_name)\n",
        "                        shutil.copy(src_path, new_dest_path)\n",
        "                    else:\n",
        "                        shutil.copy(src_path, dest_path)\n",
        "    print(f\"Processed waste_classification_data from {source_dir} \"\n",
        "          f\"to {dest_dir_root}\")\n",
        "\n",
        "\n",
        "process_waste_classification_data(\n",
        "    source_dir='./datasets/waste_classification_data',\n",
        "    dest_dir_root=output_root,\n",
        "    mapping=mappings['waste-classification-data']\n",
        ")\n",
        "\n",
        "\n",
        "# we can now do the same with the garbage_classification dataset (second of the original ones)\n",
        "# the structure is easy, inside the folder we have named subfolders, full of images\n",
        "# we can use the mapping dictionary again\n",
        "\n",
        "def process_garbage_classification(source_dir, dest_dir_root, mapping):\n",
        "    for class_folder in tqdm(listdir(source_dir), desc=\"Processing garbage_classification\"):\n",
        "        class_dir = join(source_dir, class_folder)\n",
        "        if class_folder in mapping:\n",
        "            final_class = mapping[class_folder]\n",
        "            final_class_dir = join(dest_dir_root, final_class)\n",
        "            for img_file in tqdm(listdir(class_dir), desc=f\"Processing images in {class_folder}\"):\n",
        "                src_path = join(class_dir, img_file)\n",
        "                dest_path = join(final_class_dir, img_file)\n",
        "                if os.path.exists(dest_path):\n",
        "                    # If file already exists, create a unique name\n",
        "                    base, ext = os.path.splitext(img_file)\n",
        "                    counter = 1\n",
        "                    new_name = f\"{base}_{counter}{ext}\"\n",
        "                    new_dest_path = join(final_class_dir, new_name)\n",
        "                    while os.path.exists(new_dest_path):\n",
        "                        counter += 1\n",
        "                        new_name = f\"{base}_{counter}{ext}\"\n",
        "                        new_dest_path = join(final_class_dir, new_name)\n",
        "                    shutil.copy(src_path, new_dest_path)\n",
        "                else:\n",
        "                    shutil.copy(src_path, dest_path)\n",
        "    print(f\"Processed garbage_classification from {source_dir} \"\n",
        "          f\"to {dest_dir_root}\")\n",
        "\n",
        "\n",
        "process_garbage_classification(\n",
        "    source_dir='./datasets/garbage_classification',\n",
        "    dest_dir_root=output_root,\n",
        "    mapping=mappings['garbage_classification']\n",
        ")\n",
        "\n",
        "# now new datasets\n",
        "# first one is the recyclable-and-household-waste-classification\n",
        "# it has a simple structure, in the root folder there is an  images subfolder, with another images subfolder\n",
        "# after that there are subfolders named according to the waste types\n",
        "# inside that there are two folders - \"default\" and \"real_world\". We can combine images from both folders\n",
        "# again, we will use the mapping dictionary\n",
        "\n",
        "def process_recyclable_and_household_waste_classification(source_dir, dest_dir_root, mapping):\n",
        "    images_root = join(source_dir, 'images', 'images')\n",
        "    for class_folder in tqdm(listdir(images_root), desc=\"Processing recyclable-and-household-waste-classification\"):\n",
        "        class_dir = join(images_root, class_folder)\n",
        "        if class_folder in mapping:\n",
        "            final_class = mapping[class_folder]\n",
        "            final_class_dir = join(dest_dir_root, final_class)\n",
        "            # process both \"default\" and \"real_world\" subfolders\n",
        "            for subfolder in ['default', 'real_world']:\n",
        "                subfolder_dir = join(class_dir, subfolder)\n",
        "                if os.path.exists(subfolder_dir):\n",
        "                    for img_file in listdir(subfolder_dir):\n",
        "                        src_path = join(subfolder_dir, img_file)\n",
        "                        dest_path = join(final_class_dir, img_file)\n",
        "                        if os.path.exists(dest_path):\n",
        "                            # If file already exists, create a unique name\n",
        "                            base, ext = os.path.splitext(img_file)\n",
        "                            counter = 1\n",
        "                            new_name = f\"{base}_{counter}{ext}\"\n",
        "                            new_dest_path = join(final_class_dir, new_name)\n",
        "                            while os.path.exists(new_dest_path):\n",
        "                                counter += 1\n",
        "                                new_name = f\"{base}_{counter}{ext}\"\n",
        "                                new_dest_path = join(final_class_dir, new_name)\n",
        "                            shutil.copy(src_path, new_dest_path)\n",
        "                        else:\n",
        "                            shutil.copy(src_path, dest_path)\n",
        "    print(f\"Processed recyclable-and-household-waste-classification from {source_dir} \"\n",
        "          f\"to {dest_dir_root}\")\n",
        "\n",
        "\n",
        "process_recyclable_and_household_waste_classification(\n",
        "    source_dir='./datasets/recyclable-and-household-waste-classification',\n",
        "    dest_dir_root=output_root,\n",
        "    mapping=mappings['recyclable-and-household-waste-classification']\n",
        ")\n",
        "\n",
        "# the next one is garbage-classification-12-classes\n",
        "# structure - inside the root folder there is a garbage_classification subfolder, and inside there are subfolders named according to the waste types\n",
        "# we will use the mapping dictionary again\n",
        "\n",
        "def process_garbage_classification_12_classes(source_dir, dest_dir_root, mapping):\n",
        "    garbage_root = join(source_dir, 'garbage_classification')\n",
        "    for class_folder in tqdm(listdir(garbage_root), desc=\"Processing garbage-classification-12-classes\"):\n",
        "        class_dir = join(garbage_root, class_folder)\n",
        "        if class_folder in mapping:\n",
        "            final_class = mapping[class_folder]\n",
        "            final_class_dir = join(dest_dir_root, final_class)\n",
        "            for img_file in tqdm(listdir(class_dir), desc=f\"Processing images in {class_folder}\"):\n",
        "                src_path = join(class_dir, img_file)\n",
        "                dest_path = join(final_class_dir, img_file)\n",
        "                if os.path.exists(dest_path):\n",
        "                    # If file already exists, create a unique name\n",
        "                    base, ext = os.path.splitext(img_file)\n",
        "                    counter = 1\n",
        "                    new_name = f\"{base}_{counter}{ext}\"\n",
        "                    new_dest_path = join(final_class_dir, new_name)\n",
        "                    while os.path.exists(new_dest_path):\n",
        "                        counter += 1\n",
        "                        new_name = f\"{base}_{counter}{ext}\"\n",
        "                        new_dest_path = join(final_class_dir, new_name)\n",
        "                    shutil.copy(src_path, new_dest_path)\n",
        "                else:\n",
        "                    shutil.copy(src_path, dest_path)\n",
        "    print(f\"Processed garbage-classification-12-classes from {source_dir} \"\n",
        "          f\"to {dest_dir_root}\")\n",
        "\n",
        "\n",
        "process_garbage_classification_12_classes(\n",
        "    source_dir='./datasets/garbage-classification-12-classes',\n",
        "    dest_dir_root=output_root,\n",
        "    mapping=mappings['garbage-classification-12-classes']\n",
        ")\n",
        "\n",
        "\n",
        "# next one is the garbage-classification-v2 dataset\n",
        "# structure - inside the root folder there is a garbage-dataset subfolder, and inside there are subfolders named according to the waste types\n",
        "# we will use the mapping dictionary again\n",
        "\n",
        "def process_garbage_classification_v2(source_dir, dest_dir_root, mapping):\n",
        "    garbage_root = join(source_dir, 'garbage-dataset')\n",
        "    for class_folder in tqdm(listdir(garbage_root), desc=\"Processing garbage-classification-v2\"):\n",
        "        class_dir = join(garbage_root, class_folder)\n",
        "        if class_folder in mapping:\n",
        "            final_class = mapping[class_folder]\n",
        "            final_class_dir = join(dest_dir_root, final_class)\n",
        "            for img_file in tqdm(listdir(class_dir), desc=f\"Processing images in {class_folder}\"):\n",
        "                src_path = join(class_dir, img_file)\n",
        "                dest_path = join(final_class_dir, img_file)\n",
        "                if os.path.exists(dest_path):\n",
        "                    # If file already exists, create a unique name\n",
        "                    base, ext = os.path.splitext(img_file)\n",
        "                    counter = 1\n",
        "                    new_name = f\"{base}_{counter}{ext}\"\n",
        "                    new_dest_path = join(final_class_dir, new_name)\n",
        "                    while os.path.exists(new_dest_path):\n",
        "                        counter += 1\n",
        "                        new_name = f\"{base}_{counter}{ext}\"\n",
        "                        new_dest_path = join(final_class_dir, new_name)\n",
        "                    shutil.copy(src_path, new_dest_path)\n",
        "                else:\n",
        "                    shutil.copy(src_path, dest_path)\n",
        "    print(f\"Processed garbage-classification-v2 from {source_dir} \"\n",
        "          f\"to {dest_dir_root}\")\n",
        "\n",
        "\n",
        "process_garbage_classification_v2(\n",
        "    source_dir='./datasets/garbage-classification-v2',\n",
        "    dest_dir_root=output_root,\n",
        "    mapping=mappings['garbage-classification-v2']\n",
        ")\n",
        "\n",
        "# now the garbage-dataset-classification dataset\n",
        "# structure - inside the root folder there is a Garbage_Dataset_Classification subfolder\n",
        "# inside there is an images subfolder, and inside there are subfolders named according to the waste types\n",
        "# we will use the mapping dictionary again\n",
        "\n",
        "\n",
        "def process_garbage_dataset_classification(source_dir, dest_dir_root, mapping):\n",
        "    images_root = join(source_dir, 'Garbage_Dataset_Classification', 'images')\n",
        "    for class_folder in tqdm(listdir(images_root), desc=\"Processing garbage-dataset-classification\"):\n",
        "        class_dir = join(images_root, class_folder)\n",
        "        if class_folder in mapping:\n",
        "            final_class = mapping[class_folder]\n",
        "            final_class_dir = join(dest_dir_root, final_class)\n",
        "            for img_file in tqdm(listdir(class_dir), desc=f\"Processing images in {class_folder}\"):\n",
        "                src_path = join(class_dir, img_file)\n",
        "                dest_path = join(final_class_dir, img_file)\n",
        "                if os.path.exists(dest_path):\n",
        "                    # If file already exists, create a unique name\n",
        "                    base, ext = os.path.splitext(img_file)\n",
        "                    counter = 1\n",
        "                    new_name = f\"{base}_{counter}{ext}\"\n",
        "                    new_dest_path = join(final_class_dir, new_name)\n",
        "                    while os.path.exists(new_dest_path):\n",
        "                        counter += 1\n",
        "                        new_name = f\"{base}_{counter}{ext}\"\n",
        "                        new_dest_path = join(final_class_dir, new_name)\n",
        "                    shutil.copy(src_path, new_dest_path)\n",
        "                else:\n",
        "                    shutil.copy(src_path, dest_path)\n",
        "    print(f\"Processed garbage-dataset-classification from {source_dir} \"\n",
        "          f\"to {dest_dir_root}\")\n",
        "\n",
        "\n",
        "process_garbage_dataset_classification(\n",
        "    source_dir='./datasets/garbage-dataset-classification',\n",
        "    dest_dir_root=output_root,\n",
        "    mapping=mappings['garbage-dataset-classification']\n",
        ")\n",
        "\n",
        "\n",
        "# the last one is the plastic-paper-garbage-bag-synthetic-images dataset\n",
        "# inside the root folder there is a \"Bag Classes\" subfolder, and inside another \"Bag Classes\" subfolder\n",
        "# inside there are subfolders named according to the bag types\n",
        "# we will use the mapping dictionary to map the bag types to final classes\n",
        "\n",
        "def process_plastic_paper_garbage_bag_synthetic_images(source_dir, dest_dir_root, mapping):\n",
        "    bags_root = join(source_dir, 'Bag Classes', 'Bag Classes')\n",
        "    for class_folder in tqdm(listdir(bags_root), desc=\"Processing plastic-paper-garbage-bag-synthetic-images\"):\n",
        "        class_dir = join(bags_root, class_folder)\n",
        "        if class_folder in mapping:\n",
        "            final_class = mapping[class_folder]\n",
        "            final_class_dir = join(dest_dir_root, final_class)\n",
        "            for img_file in tqdm(listdir(class_dir), desc=f\"Processing images in {class_folder}\"):\n",
        "                src_path = join(class_dir, img_file)\n",
        "                dest_path = join(final_class_dir, img_file)\n",
        "                if os.path.exists(dest_path):\n",
        "                    # If file already exists, create a unique name\n",
        "                    base, ext = os.path.splitext(img_file)\n",
        "                    counter = 1\n",
        "                    new_name = f\"{base}_{counter}{ext}\"\n",
        "                    new_dest_path = join(final_class_dir, new_name)\n",
        "                    while os.path.exists(new_dest_path):\n",
        "                        counter += 1\n",
        "                        new_name = f\"{base}_{counter}{ext}\"\n",
        "                        new_dest_path = join(final_class_dir, new_name)\n",
        "                    shutil.copy(src_path, new_dest_path)\n",
        "                else:\n",
        "                    shutil.copy(src_path, dest_path)\n",
        "    print(f\"Processed plastic-paper-garbage-bag-synthetic-images from {source_dir} \"\n",
        "          f\"to {dest_dir_root}\")\n",
        "\n",
        "\n",
        "process_plastic_paper_garbage_bag_synthetic_images(\n",
        "    source_dir='./datasets/plastic-paper-garbage-bag-synthetic-images',\n",
        "    dest_dir_root=output_root,\n",
        "    mapping=mappings['plastic-paper-garbage-bag-synthetic-images']\n",
        ")"
      ],
      "metadata": {
        "id": "UkSXfiW4nw4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import random\n",
        "from os.path import join\n",
        "\n",
        "\n",
        "# set random seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Define the classes (same as in the combination script)\n",
        "final_classes = {\n",
        "    'organic': 0,\n",
        "    'battery': 1,\n",
        "    'glass': 2,\n",
        "    'metal': 3,\n",
        "    'paper': 4,\n",
        "    'cardboard': 5,\n",
        "    'plastic': 6,\n",
        "    'textiles': 7,\n",
        "    'trash': 8,\n",
        "}\n",
        "\n",
        "\n",
        "# Paths\n",
        "dataset_root = './datasets/combined_waste_dataset'\n",
        "\n",
        "# Split ratios\n",
        "train_ratio = 0.7\n",
        "val_ratio = 0.15\n",
        "test_ratio = 0.15\n",
        "\n",
        "# Ensure ratios add up to 1\n",
        "assert train_ratio + val_ratio + test_ratio == 1.0\n",
        "\n",
        "\n",
        "def create_csv_files(out_root):\n",
        "    \"\"\"Create CSV files for train, val, test splits.\"\"\"\n",
        "    os.makedirs(out_root, exist_ok=True)\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        csv_path = join(out_root, f'{split}.csv')\n",
        "        with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(['image_path', 'class'])\n",
        "\n",
        "\n",
        "def split_and_write_csvs(comb_root, out_root, classes,\n",
        "                         train_r, val_r):\n",
        "    \"\"\"Split images and write to CSV files.\"\"\"\n",
        "    csv_files = {}\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        csv_path = join(out_root, f'{split}.csv')\n",
        "        csv_files[split] = open(csv_path, 'a', newline='', encoding='utf-8')\n",
        "\n",
        "    writers = {split: csv.writer(f) for split, f in csv_files.items()}\n",
        "\n",
        "    try:\n",
        "        for cls in classes.keys():\n",
        "            cls_dir = join(comb_root, cls)\n",
        "            if not os.path.exists(cls_dir):\n",
        "                print(f\"Warning: Class directory {cls_dir} does not exist. \"\n",
        "                      \"Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Get all image files\n",
        "            images = [f for f in os.listdir(cls_dir)\n",
        "                      if os.path.isfile(join(cls_dir, f))]\n",
        "            if not images:\n",
        "                print(f\"Warning: No images found in {cls_dir}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Shuffle the images\n",
        "            random.shuffle(images)\n",
        "            n = len(images)\n",
        "\n",
        "            # Calculate split indices\n",
        "            train_end = int(train_r * n)\n",
        "            val_end = int((train_r + val_r) * n)\n",
        "\n",
        "            train_imgs = images[:train_end]\n",
        "            val_imgs = images[train_end:val_end]\n",
        "            test_imgs = images[val_end:]\n",
        "\n",
        "            # Write to CSVs\n",
        "            splits_data = [('train', train_imgs), ('val', val_imgs),\n",
        "                           ('test', test_imgs)]\n",
        "            for split, imgs in splits_data:\n",
        "                for img in imgs:\n",
        "                    rel_path = join(cls, img)\n",
        "                    writers[split].writerow([rel_path, cls])\n",
        "\n",
        "            print(f\"Class {cls}: {len(train_imgs)} train, \"\n",
        "                  f\"{len(val_imgs)} val, {len(test_imgs)} test images.\")\n",
        "    finally:\n",
        "        for f in csv_files.values():\n",
        "            f.close()\n"
      ],
      "metadata": {
        "id": "LV3Z66jmnw66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create CSV files\n",
        "create_csv_files(dataset_root)\n",
        "\n",
        "# Split and write\n",
        "split_and_write_csvs(dataset_root, dataset_root, final_classes,\n",
        "                      train_ratio, val_ratio)\n",
        "\n",
        "print(\"Dataset splitting completed!\")"
      ],
      "metadata": {
        "id": "UthViMn_nw8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We can finally create a combined dataset class and test it\n"
      ],
      "metadata": {
        "id": "tgDHkRk8oRty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combined Waste Dataset for Multiclass Classification\n",
        "# Uses CSV splits generated by create_dataset_splits.py\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "\n",
        "# Define the classes (same as in the combination script)\n",
        "final_classes = {\n",
        "    'organic': 0,\n",
        "    'battery': 1,\n",
        "    'glass': 2,\n",
        "    'metal': 3,\n",
        "    'paper': 4,\n",
        "    'cardboard': 5,\n",
        "    'plastic': 6,\n",
        "    'textiles': 7,\n",
        "    'trash': 8,\n",
        "}\n",
        "\n",
        "class CombinedWasteDatasetMulti(Dataset):\n",
        "    def __init__(self, root_dir=\"./datasets/combined_waste_dataset\",\n",
        "                 split='train', transform=None):\n",
        "        \"\"\"\n",
        "        Multiclass waste dataset using CSV splits.\n",
        "\n",
        "        Args:\n",
        "            root_dir (str): Path to the combined waste dataset directory.\n",
        "            split (str): 'train', 'val', or 'test'.\n",
        "            transform: Transformations to apply to images.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.classes = list(final_classes.keys())\n",
        "        self.class_to_idx = final_classes\n",
        "        self.data = []\n",
        "        self._load_data()\n",
        "\n",
        "    def _load_data(self):\n",
        "        csv_path = os.path.join(self.root_dir, f'{self.split}.csv')\n",
        "        if not os.path.exists(csv_path):\n",
        "            raise FileNotFoundError(f\"CSV file {csv_path} not found. \"\n",
        "                                    \"Run create_dataset_splits.py first.\")\n",
        "\n",
        "        df = pd.read_csv(csv_path)\n",
        "        for _, row in df.iterrows():\n",
        "            img_path = os.path.join(self.root_dir, row['image_path'])\n",
        "            label = self.class_to_idx[row['class']]\n",
        "            self.data.append((img_path, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.data[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "QP5f3Bs4np13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the dataset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from collections import Counter\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "dataset_root = './datasets/combined_waste_dataset'\n",
        "\n",
        "for split in ['train', 'val', 'test']:\n",
        "    dataset = CombinedWasteDatasetMulti(\n",
        "        root_dir=dataset_root, split=split, transform=transform)\n",
        "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "    print(f\"Split: {split}, Total Samples: {len(dataset)}\")\n",
        "\n",
        "    # Count images per class\n",
        "    labels = [label for _, label in dataset.data]\n",
        "    counts = Counter(labels)\n",
        "    print(\"  Class distribution:\")\n",
        "    for cls_idx in sorted(counts.keys()):\n",
        "        cls_name = dataset.classes[cls_idx]\n",
        "        print(f\"    {cls_name} ({cls_idx}): {counts[cls_idx]} images\")\n",
        "\n",
        "    if len(dataset) > 0:\n",
        "        # Test one batch\n",
        "        for images, labels_batch in dataloader:\n",
        "            print(f\"  Batch shape: {images.shape}, Labels: {labels_batch[0:5] } (...)\")\n",
        "            break\n"
      ],
      "metadata": {
        "id": "8XLBzIdFnfpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New stuff"
      ],
      "metadata": {
        "id": "FU7lWpVPrGcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us first validate our previous work.\n",
        "By doing all combining of datasets we might have inadvertently introduced some problems.\n",
        "Let's check that our datasets are still valid, that we are not \"poisinoning\" our data by accident.\n",
        "\n",
        "Let us start with something simple - checking for duplicates.\n",
        "We can do this by either looking at our data and seeing if there are any similar images OR\n",
        "we can start simple - checking raw data for duplicates."
      ],
      "metadata": {
        "id": "VxkwHLbHrNPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this code searches for duplicate images in a dataset by computing their MD5 hashes\n",
        "# this is the easiest way to find identical files, without using \"semantic\" methods\n",
        "# where we would look for visually similar images.\n",
        "# Algorithm: Compute MD5 hash of raw file bytes for each image.\n",
        "# Duplicates are files with identical hashes.\n",
        "\n",
        "import os\n",
        "import hashlib\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import concurrent.futures\n",
        "import csv\n",
        "\n",
        "\n",
        "def process_file(root_dir, rel_path):\n",
        "    \"\"\"\n",
        "    Process a single file to compute its hash.\n",
        "\n",
        "    Args:\n",
        "        root_dir (str): Root directory of the dataset.\n",
        "        rel_path (str): Relative path to the file.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (hash, rel_path) or (None, rel_path) on error.\n",
        "    \"\"\"\n",
        "    full_path = os.path.join(root_dir, rel_path)\n",
        "    try:\n",
        "        with open(full_path, 'rb') as f:\n",
        "            data = f.read()\n",
        "        img_hash = hashlib.md5(data).hexdigest()\n",
        "        return img_hash, rel_path\n",
        "    except (IOError, OSError) as e:\n",
        "        print(f\"Error processing {rel_path}: {e}\")\n",
        "        return None, rel_path\n",
        "\n",
        "\n",
        "def compute_image_hashes(root_dir):\n",
        "    \"\"\"\n",
        "    Compute MD5 hashes for all images in the dataset.\n",
        "    This algorithm reads the raw file bytes and computes an MD5 hash\n",
        "    to detect exact duplicates. Images with the same hash are identical.\n",
        "\n",
        "    Args:\n",
        "        root_dir (str): Root directory of the dataset.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary where keys are MD5 hashes and values are lists\n",
        "              of file paths with that hash.\n",
        "    \"\"\"\n",
        "    hash_dict = defaultdict(list)\n",
        "    image_files = []\n",
        "\n",
        "    print(\"Collecting image files...\")\n",
        "    # Collect all image files\n",
        "    for subdir, _, files in os.walk(root_dir):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.png', '.jpg', '.jpeg',\n",
        "                                      '.bmp', '.gif')):\n",
        "                rel_path = os.path.relpath(\n",
        "                    os.path.join(subdir, file), root_dir\n",
        "                )\n",
        "                image_files.append(rel_path)\n",
        "\n",
        "    print(f\"Found {len(image_files)} image files. Processing...\")\n",
        "\n",
        "    # Process images in parallel with progress bar\n",
        "    # this is not ideal, but works for now\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        results = list(tqdm(\n",
        "            executor.map(lambda fp: process_file(root_dir, fp), image_files),\n",
        "            total=len(image_files),\n",
        "            desc=\"Processing images\"\n",
        "        ))\n",
        "\n",
        "    # Collect results\n",
        "    # simplest possible solution - just group by hash\n",
        "    for img_hash, filepath in results:\n",
        "        if img_hash:\n",
        "            hash_dict[img_hash].append(filepath)\n",
        "\n",
        "    return hash_dict\n",
        "\n",
        "\n",
        "def report_duplicates(hash_dict, output_file, csv_file):\n",
        "    \"\"\"\n",
        "    Report duplicate images found and save to file and CSV.\n",
        "\n",
        "    Args:\n",
        "        hash_dict (dict): Dictionary from find_duplicate_images.\n",
        "        output_file (str): Path to the output text file.\n",
        "        csv_file (str): Path to the output CSV file for files to delete.\n",
        "    \"\"\"\n",
        "    total_images = sum(len(files) for files in hash_dict.values())\n",
        "    unique_images = sum(1 for files in hash_dict.values() if len(files) == 1)\n",
        "\n",
        "    # Count duplicate images:\n",
        "    duplicate_images = sum(\n",
        "        len(files) - 1 for files in hash_dict.values() if len(files) > 1\n",
        "    )\n",
        "\n",
        "    duplicates_found = duplicate_images > 0\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"Summary:\\n\")\n",
        "        f.write(f\"Total images processed: {total_images}\\n\")\n",
        "        f.write(f\"Unique images: {unique_images}\\n\")\n",
        "        f.write(f\"Duplicate images: {duplicate_images}\\n\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "        if duplicates_found:\n",
        "            f.write(\"Duplicate groups:\\n\")\n",
        "            group_num = 1\n",
        "            for hash_val, files in hash_dict.items():\n",
        "                if len(files) > 1:  # Duplicates: multiple files with same hash\n",
        "                    f.write(f\"Duplicate group {group_num} \"\n",
        "                            f\"(hash: {hash_val}):\\n\")\n",
        "                    # Sort files for consistency, keep the first\n",
        "                    sorted_files = sorted(files)\n",
        "                    keep_file = sorted_files[0]\n",
        "                    delete_files = sorted_files[1:]\n",
        "                    f.write(f\"  Keep: {keep_file}\\n\")\n",
        "                    for del_file in delete_files:\n",
        "                        f.write(f\"  Delete: {del_file}\\n\")\n",
        "                    f.write(\"\\n\")\n",
        "                    group_num += 1\n",
        "        else:\n",
        "            f.write(\"No duplicate images found.\\n\")\n",
        "\n",
        "    # Write CSV for files to delete (duplicates, excluding one per group)\n",
        "    with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['file_path'])  # Header\n",
        "        for hash_val, files in hash_dict.items():\n",
        "            if len(files) > 1:  # Duplicates found here\n",
        "                sorted_files = sorted(files)\n",
        "                for del_file in sorted_files[1:]:  # Skip the first (keep)\n",
        "                    writer.writerow([del_file])\n",
        "\n",
        "    print(f\"Report saved to {output_file} and {csv_file}\")"
      ],
      "metadata": {
        "id": "9l8d_LSCrS7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, we have created a simple script to find duplicate images in our dataset.\n",
        "Calculating hashes is relatively fast, so we can run this on our combined dataset\n",
        "\n",
        "Let's run it and see if we have any duplicates."
      ],
      "metadata": {
        "id": "mEToeNeSrWCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_root = './datasets/combined_waste_dataset'\n",
        "\n",
        "if not os.path.exists(dataset_root):\n",
        "    print(f\"Dataset directory {dataset_root} does not exist.\")\n",
        "    exit(1)\n",
        "\n",
        "print(\"Scanning for duplicate images...\")\n",
        "hashes = compute_image_hashes(dataset_root)\n",
        "report_duplicates(hashes, 'duplicates_report.txt',\n",
        "                    'duplicates_to_delete.csv')"
      ],
      "metadata": {
        "id": "WdBePFCsrgTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, by looking at the report we can see that we have a lot of duplicated images.\n",
        "Does this fully invalidate our results? Unfortunately, yes.\n",
        "\n",
        "If the same image appears in both training and validation sets,\n",
        "our validation accuracy will be artificially high, as the model has already seen those images during training.\n",
        "We should remove duplicates from the dataset and try to train again."
      ],
      "metadata": {
        "id": "TVrCK23qri7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "\n",
        "\n",
        "def clean_dataset(csv_file, dataset_root):\n",
        "    \"\"\"\n",
        "    Remove duplicate files listed in the CSV.\n",
        "\n",
        "    Args:\n",
        "        csv_file (str): Path to the CSV file with files to delete.\n",
        "        dataset_root (str): Root directory of the dataset.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(csv_file):\n",
        "        print(f\"CSV file {csv_file} does not exist.\")\n",
        "        return\n",
        "\n",
        "    if not os.path.exists(dataset_root):\n",
        "        print(f\"Dataset root {dataset_root} does not exist.\")\n",
        "        return\n",
        "\n",
        "    deleted_count = 0\n",
        "    with open(csv_file, 'r', encoding='utf-8') as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            rel_path = row['file_path']\n",
        "            full_path = os.path.join(dataset_root, rel_path)\n",
        "            if os.path.exists(full_path):\n",
        "                os.remove(full_path)\n",
        "                print(f\"Deleted: {full_path}\")\n",
        "                deleted_count += 1\n",
        "            else:\n",
        "                print(f\"File not found: {full_path}\")\n",
        "\n",
        "    print(f\"Cleanup complete. Deleted {deleted_count} files.\")"
      ],
      "metadata": {
        "id": "XZ72AnWOrmcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path = 'duplicates_to_delete.csv'\n",
        "root_dir = './datasets/combined_waste_dataset'\n",
        "clean_dataset(csv_path, root_dir)"
      ],
      "metadata": {
        "id": "RuJlrvytro42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running the code we lost a lot of images from our dataset.\n",
        "But at least we now have a better source of data.\n",
        "\n",
        "This is, unfortunately, a common problem when combining datasets from multiple sources.\n",
        "There is even a saying in data science: \"Garbage in, garbage out\".\n",
        "\n",
        "We always MUST make sure our data is clean and valid before training any models.\n",
        "This dataset is quite small, for now, so doing clean-up is still quite fast.\n",
        "But when workng with larger datasets (e.g., millions of images),\n",
        "we need to be very careful and efficient in our data validation and cleaning steps.\n",
        "\n",
        "This will be even more important when working with real-world data, which is often messy and full of errors.\n",
        "And if we will try do work with Large Language Models (LLMs) in the future, data quality will be even more critical.\n",
        "Especially when trying to compare using \"benchmarks\" for LLMs, where leaks can invalidate results."
      ],
      "metadata": {
        "id": "8qMIWLdJrvIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, we can try to re-create our dataset splits again now that we have removed duplicates."
      ],
      "metadata": {
        "id": "U3IVBw-orzDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import random\n",
        "from os.path import join\n",
        "\n",
        "\n",
        "# set random seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Define the classes (same as in the combination script)\n",
        "final_classes = {\n",
        "    'organic': 0,\n",
        "    'battery': 1,\n",
        "    'glass': 2,\n",
        "    'metal': 3,\n",
        "    'paper': 4,\n",
        "    'cardboard': 5,\n",
        "    'plastic': 6,\n",
        "    'textiles': 7,\n",
        "    'trash': 8,\n",
        "}\n",
        "\n",
        "\n",
        "# Paths\n",
        "dataset_root = './datasets/combined_waste_dataset'\n",
        "\n",
        "# Split ratios\n",
        "train_ratio = 0.7\n",
        "val_ratio = 0.15\n",
        "test_ratio = 0.15\n",
        "\n",
        "# Ensure ratios add up to 1\n",
        "assert train_ratio + val_ratio + test_ratio == 1.0\n",
        "\n",
        "\n",
        "def create_csv_files(out_root):\n",
        "    \"\"\"Create CSV files for train, val, test splits.\"\"\"\n",
        "    os.makedirs(out_root, exist_ok=True)\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        csv_path = join(out_root, f'{split}.csv')\n",
        "        with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(['image_path', 'class'])\n",
        "\n",
        "\n",
        "def split_and_write_csvs(comb_root, out_root, classes,\n",
        "                         train_r, val_r):\n",
        "    \"\"\"Split images and write to CSV files.\"\"\"\n",
        "    csv_files = {}\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        csv_path = join(out_root, f'{split}.csv')\n",
        "        csv_files[split] = open(csv_path, 'a', newline='', encoding='utf-8')\n",
        "\n",
        "    writers = {split: csv.writer(f) for split, f in csv_files.items()}\n",
        "\n",
        "    try:\n",
        "        for cls in classes.keys():\n",
        "            cls_dir = join(comb_root, cls)\n",
        "            if not os.path.exists(cls_dir):\n",
        "                print(f\"Warning: Class directory {cls_dir} does not exist. \"\n",
        "                      \"Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Get all image files\n",
        "            images = [f for f in os.listdir(cls_dir)\n",
        "                      if os.path.isfile(join(cls_dir, f))]\n",
        "            if not images:\n",
        "                print(f\"Warning: No images found in {cls_dir}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Shuffle the images\n",
        "            random.shuffle(images)\n",
        "            n = len(images)\n",
        "\n",
        "            # Calculate split indices\n",
        "            train_end = int(train_r * n)\n",
        "            val_end = int((train_r + val_r) * n)\n",
        "\n",
        "            train_imgs = images[:train_end]\n",
        "            val_imgs = images[train_end:val_end]\n",
        "            test_imgs = images[val_end:]\n",
        "\n",
        "            # Write to CSVs\n",
        "            splits_data = [('train', train_imgs), ('val', val_imgs),\n",
        "                           ('test', test_imgs)]\n",
        "            for split, imgs in splits_data:\n",
        "                for img in imgs:\n",
        "                    rel_path = join(cls, img)\n",
        "                    writers[split].writerow([rel_path, cls])\n",
        "\n",
        "            print(f\"Class {cls}: {len(train_imgs)} train, \"\n",
        "                  f\"{len(val_imgs)} val, {len(test_imgs)} test images.\")\n",
        "    finally:\n",
        "        for f in csv_files.values():\n",
        "            f.close()"
      ],
      "metadata": {
        "id": "NzCdocdzr0Fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create CSV files\n",
        "create_csv_files(dataset_root)\n",
        "\n",
        "# Split and write\n",
        "split_and_write_csvs(dataset_root, dataset_root, final_classes,\n",
        "                        train_ratio, val_ratio)\n",
        "\n",
        "print(\"Dataset splitting completed!\")"
      ],
      "metadata": {
        "id": "YGm8eqpIr6S0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The dataset class can have the exact same code as before, no changes needed.\n",
        "\n",
        "# Combined Waste Dataset for Multiclass Classification\n",
        "# Uses CSV splits generated by create_dataset_splits.py\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "\n",
        "# Define the classes (same as in the combination script)\n",
        "final_classes = {\n",
        "    'organic': 0,\n",
        "    'battery': 1,\n",
        "    'glass': 2,\n",
        "    'metal': 3,\n",
        "    'paper': 4,\n",
        "    'cardboard': 5,\n",
        "    'plastic': 6,\n",
        "    'textiles': 7,\n",
        "    'trash': 8,\n",
        "}\n",
        "\n",
        "class CombinedWasteDatasetMulti(Dataset):\n",
        "    def __init__(self, root_dir=\"./datasets/combined_waste_dataset\",\n",
        "                 split='train', transform=None):\n",
        "        \"\"\"\n",
        "        Multiclass waste dataset using CSV splits.\n",
        "\n",
        "        Args:\n",
        "            root_dir (str): Path to the combined waste dataset directory.\n",
        "            split (str): 'train', 'val', or 'test'.\n",
        "            transform: Transformations to apply to images.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.classes = list(final_classes.keys())\n",
        "        self.class_to_idx = final_classes\n",
        "        self.data = []\n",
        "        self._load_data()\n",
        "\n",
        "    def _load_data(self):\n",
        "        csv_path = os.path.join(self.root_dir, f'{self.split}.csv')\n",
        "        if not os.path.exists(csv_path):\n",
        "            raise FileNotFoundError(f\"CSV file {csv_path} not found. \"\n",
        "                                    \"Run create_dataset_splits.py first.\")\n",
        "\n",
        "        df = pd.read_csv(csv_path)\n",
        "        for _, row in df.iterrows():\n",
        "            img_path = os.path.join(self.root_dir, row['image_path'])\n",
        "            label = self.class_to_idx[row['class']]\n",
        "            self.data.append((img_path, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.data[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "T-jOhRGEr-es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the dataset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from collections import Counter\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "dataset_root = './datasets/combined_waste_dataset'\n",
        "\n",
        "for split in ['train', 'val', 'test']:\n",
        "    dataset = CombinedWasteDatasetMulti(\n",
        "        root_dir=dataset_root, split=split, transform=transform)\n",
        "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "    print(f\"Split: {split}, Total Samples: {len(dataset)}\")\n",
        "\n",
        "    # Count images per class\n",
        "    labels = [label for _, label in dataset.data]\n",
        "    counts = Counter(labels)\n",
        "    print(\"  Class distribution:\")\n",
        "    for cls_idx in sorted(counts.keys()):\n",
        "        cls_name = dataset.classes[cls_idx]\n",
        "        print(f\"    {cls_name} ({cls_idx}): {counts[cls_idx]} images\")\n",
        "\n",
        "    if len(dataset) > 0:\n",
        "        # Test one batch\n",
        "        for images, labels_batch in dataloader:\n",
        "            print(f\"  Batch shape: {images.shape}, Labels: {labels_batch[0:5] } (...)\")\n",
        "            break"
      ],
      "metadata": {
        "id": "kRPFZ_VNsBMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, now we can think about the architecture of our model.\n",
        "Our first attempt was an architecture \"inspired\" by VGG network.\n",
        "\n",
        "But it was neither a direct copy nor something based on gained knowledge. It was more of a guess.\n",
        "How would we know we picked a good architecture?\n",
        "\n",
        "We can see from literature, that Architecutures are usualy designed through a combination of:\n",
        "- Empirical testing\n",
        "- Prior knowledge\n",
        "- Automated search (Neural Architecture Search - NAS)\n",
        "\n",
        "Let's try do do something similiar here.\n",
        "We can try to implement our network in a modular way, so that we can easily change its architecture.\n",
        "\n",
        "We can define building blocks for our network, such as convolutional layers, activation functions, and pooling layers, and then compose them to create the final architecture.\n",
        "\n",
        "This way, we can easily experiment with different configurations and find the best-performing architecture for our task.\n",
        "\n",
        "For now let's stick to things that are \"inspired\" by literature. We know what worked previously AND was simple:\n",
        "- AlexNet\n",
        "- VGG\n",
        "\n",
        "Let's try to implement modular versions of these architectures and see how they perform on our dataset.\n",
        "Knowing how to build modular architectures is a useful skill, as basically anything more complicated will be just a combination of simpler blocks.\n",
        "So all future architectures you will encounter will be created like this.\n",
        "\n",
        "\n",
        "\n",
        "The source of VGG architecture:\n",
        "https://arxiv.org/pdf/1409.1556\n",
        "\n",
        "We will get inspirations from it, BUT it has no drawing of the architecture itself.\n",
        "Fortunately there are multiple articles that describe it in more detail, e.g.:\n",
        "https://neurohive.io/en/popular-networks/vgg16/\n",
        "https://lekhuyen.medium.com/an-overview-of-vgg16-and-nin-models-96e4bf398484\n",
        "\n",
        "AND many other articles, like:\n",
        "https://www.researchgate.net/figure/GG-16-network-architecture-for-feature-extraction_fig1_335184836\n",
        "\n",
        "(These are just exaples, to visualize the architecture)\n",
        "\n"
      ],
      "metadata": {
        "id": "NDjGQWTksFPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "from pytorch_lightning import LightningModule, Trainer, LightningDataModule\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
        "from torchsummary import summary\n",
        "import pytorch_lightning as pl\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import os\n",
        "\n",
        "# Set seed for reproducibility\n",
        "pl.seed_everything(42)\n"
      ],
      "metadata": {
        "id": "-yg6GtgAsGd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentations for training\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # ImageNet normalization\n",
        "])\n",
        "\n",
        "# Validation and test transforms\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ],
      "metadata": {
        "id": "VWVtirMvsIXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WasteDataModuleMulti(LightningDataModule):\n",
        "    def __init__(self, root_dir='datasets/combined_waste_dataset',\n",
        "                 batch_size=32):\n",
        "        super().__init__()\n",
        "        self.root_dir = root_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.num_classes = 9  # organic, battery, glass, metal, paper, cardboard, plastic, textiles, trash\n",
        "        self.class_weights = None\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.train_dataset = CombinedWasteDatasetMulti(\n",
        "            root_dir=self.root_dir, split='train', transform=train_transform\n",
        "        )\n",
        "        self.val_dataset = CombinedWasteDatasetMulti(\n",
        "            root_dir=self.root_dir, split='val', transform=val_test_transform\n",
        "        )\n",
        "        self.test_dataset = CombinedWasteDatasetMulti(\n",
        "            root_dir=self.root_dir, split='test', transform=val_test_transform\n",
        "        )\n",
        "\n",
        "        self.num_workers = os.cpu_count() - 1\n",
        "\n",
        "        # Compute sample weights for weighted random sampling\n",
        "        # Now why would we do that?\n",
        "        # In imbalanced datasets, some classes may be underrepresented in the training data.\n",
        "        # By assigning higher weights to these classes, we can ensure that the model pays more attention to them during training.\n",
        "        # This can help improve the model's performance on minority classes and lead to a more balanced overall performance.\n",
        "\n",
        "        # In \"statistical\" terms we are trying to reduce the variance of our estimator by ensuring that all classes are adequately represented in each batch.\n",
        "        # In Easy english - because our dataset is imbalanced, we want to make sure that during training, the model sees enough examples from all classes.\n",
        "        # otherwise the probability of batches without any samples from minority classes is high, and the model will not learn to recognize them well.\n",
        "\n",
        "        # This approach Creates class-balanced batches for the model to train on,\n",
        "        # which changes the actual data distribution seen by the model during training.\n",
        "\n",
        "\n",
        "        if self.class_weights is not None:\n",
        "            self.sample_weights = [self.class_weights[label].item() for _, label in self.train_dataset.data]\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        if hasattr(self, 'sample_weights'):\n",
        "            # Create a weighted random sampler for stratified sampling\n",
        "            # replacement = True to allow sampling with replacement\n",
        "            # as usual - more info in the docs:\n",
        "            # https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.WeightedRandomSampler\n",
        "            # if replacement = False -> without replacement - when a sample index is drawn for a row, it cannot be drawn again for that row\n",
        "            sampler = WeightedRandomSampler(self.sample_weights, len(self.train_dataset), replacement=True)\n",
        "\n",
        "            return DataLoader(self.train_dataset, batch_size=self.batch_size,\n",
        "                              sampler=sampler, num_workers=self.num_workers)\n",
        "        else:\n",
        "            return DataLoader(self.train_dataset, batch_size=self.batch_size,\n",
        "                              shuffle=True, num_workers=self.num_workers)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset, batch_size=self.batch_size,\n",
        "                          shuffle=False, num_workers=self.num_workers)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_dataset, batch_size=self.batch_size,\n",
        "                          shuffle=False, num_workers=self.num_workers)"
      ],
      "metadata": {
        "id": "7iiPWEtrsIZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ParametricCNN(LightningModule):\n",
        "    def __init__(self, num_classes=9, class_weights=None, architecture_config=None):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Default architecture config (our first, \"VGG-like\")\n",
        "        if architecture_config is None:\n",
        "            architecture_config = {\n",
        "                'blocks': [\n",
        "                    {'filters': 64, 'convs': 2},\n",
        "                    {'filters': 128, 'convs': 2},\n",
        "                    {'filters': 256, 'convs': 2},\n",
        "                ],\n",
        "                'dropout_conv': 0.25,\n",
        "                'fc_units': 512,\n",
        "                'dropout_fc': 0.5\n",
        "            }\n",
        "\n",
        "        self.architecture_config = architecture_config\n",
        "\n",
        "        # we can start building the model now\n",
        "        # this is not the most elegant way to do it, but it is straightforward and clear\n",
        "        # this is also not super flexible, it does not allow us to build any possible architecture,\n",
        "        # but it is not supposed to\n",
        "\n",
        "        # almost every architecture you will see will work like that\n",
        "        # just a lot of them will be more complex (ResNet, DenseNet, etc.)\n",
        "        # however, the basic idea is still the same - we have a configurtion that defines the architecture\n",
        "        # and we build the model based on that configuration, using simple \"factory\" pattern,\n",
        "        # creating our network from blocks, defined in the configuration\n",
        "        # this is especially true when creating a Sequential / linear stack of layers\n",
        "\n",
        "\n",
        "        # Build convolutional layers\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for block in architecture_config['blocks']:\n",
        "            for _ in range(block['convs']):\n",
        "                layers.append(nn.Conv2d(in_channels, block['filters'], kernel_size=3, padding=1))\n",
        "                layers.append(nn.BatchNorm2d(block['filters']))\n",
        "                layers.append(nn.ReLU())\n",
        "                in_channels = block['filters']\n",
        "            layers.append(nn.MaxPool2d(2, 2))\n",
        "            layers.append(nn.Dropout(architecture_config['dropout_conv']))\n",
        "\n",
        "        layers.append(nn.Flatten())\n",
        "\n",
        "        # Compute the flatten size dynamically\n",
        "        # otherwise we would have to hardcode it, which would be almost impossible to do\n",
        "\n",
        "        # let's use a trick - \"run\" the model we have so far on a dummy input\n",
        "        # we just have to be careful to not track gradients here\n",
        "        # that is why we use torch.inference_mode()\n",
        "        # TODO: what is the difference between torch.no_grad() and torch.inference_mode()?\n",
        "        # This was your previous homework, so you should know the answer already, though it is technical and subtle.\n",
        "        # Hint: the main difference is about creating tensor - one mode makes them impossible to use in the \"grad\" mode\n",
        "        # TODO: read this: https://docs.pytorch.org/docs/stable/notes/autograd.html#grad-modes\n",
        "        with torch.inference_mode():\n",
        "            dummy_input = torch.zeros(1, 3, 224, 224)\n",
        "            output = torch.tensor(dummy_input) # so we will have a tensor to save results into\n",
        "\n",
        "            for layer in layers[:-1]:  # Exclude Flatten for now, though it doesn't matter really, Flatten does not change number of element\n",
        "                                       # (TODO: try it yourself); It is just so we can train accessing any part of the network we want\n",
        "                output = layer(output)\n",
        "            self.flatten_size = output.numel() # numel = number of elements in the tensor\n",
        "\n",
        "        # Add fully connected layers\n",
        "        layers.append(nn.Linear(self.flatten_size, architecture_config['fc_units']))\n",
        "        layers.append(nn.BatchNorm1d(architecture_config['fc_units']))\n",
        "        layers.append(nn.ReLU())\n",
        "        layers.append(nn.Dropout(architecture_config['dropout_fc']))\n",
        "        layers.append(nn.Linear(architecture_config['fc_units'], num_classes))\n",
        "\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "        # Loss function\n",
        "        # we are using class weights here to address class imbalance\n",
        "        # this makes sure, that misclassifying samples from minority classes\n",
        "        # is penalized more than misclassifying samples from majority classes\n",
        "\n",
        "        # The gradient steps for minority class samples are magnified, making the model learn to prioritize correctly classifying these samples.\n",
        "        # The batches themselves are still drawn from the imbalanced dataset distribution.\n",
        "        self.loss_fn = nn.CrossEntropyLoss(weight=class_weights) if class_weights is not None else nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "        # If we look at our code hard enough we will notice, that we are using BOTH\n",
        "        # the class weights and the weighted random sampler to address the class imbalance issue.\n",
        "        # Now, is it a good idea to use both of them at the same time?\n",
        "        # As always - it depends. It is, unfortunately, a nuanced topic that depends on the dataset and the problem.\n",
        "\n",
        "        # By using both we are effectively training on class-balanced batches (due to the Sampler) where the minority class samples' misclassification\n",
        "        # still receives a higher penalty (due to the weighted loss). This combination can be aggressive and might\n",
        "        # be useful for highly severe imbalance, but it could also lead to overfitting to the minority class if the weights are too extreme.\n",
        "\n",
        "\n",
        "        # In some cases, using both can lead to better performance, as the sampler ensures that\n",
        "        # the model sees a balanced representation of classes during training, while the loss function\n",
        "        # emphasizes the importance of minority classes.\n",
        "        # However, in other cases, it might lead to overcompensation for minority classes,\n",
        "        # causing the model to perform poorly on majority classes.\n",
        "        # It is often a good idea to experiment with both approaches separately and together\n",
        "        # to see what works best for your specific use case.\n",
        "\n",
        "        # TODO: experiment with using only one of these techniques at a time, and compare the results.\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        images, labels = batch\n",
        "        outputs = self(images)\n",
        "        loss = self.loss_fn(outputs, labels)\n",
        "        self.log('train_loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        images, labels = batch\n",
        "        outputs = self(images)\n",
        "        loss = self.loss_fn(outputs, labels)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        acc = (predicted == labels).float().mean()\n",
        "        self.log('val_loss', loss)\n",
        "        self.log('val_acc', acc)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        images, labels = batch\n",
        "        outputs = self(images)\n",
        "        loss = self.loss_fn(outputs, labels)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        acc = (predicted == labels).float().mean()\n",
        "        self.log('test_loss', loss)\n",
        "        self.log('test_acc', acc)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.1)\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": {\n",
        "                \"scheduler\": scheduler,\n",
        "                \"monitor\": \"val_loss\",\n",
        "            },\n",
        "        }\n"
      ],
      "metadata": {
        "id": "w8-KR2QSsIeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute class weights for imbalanced dataset\n",
        "def compute_class_weights(dataset):\n",
        "    labels = [label for _, label in dataset.data]\n",
        "    class_counts = np.bincount(labels, minlength=9)\n",
        "    total_samples = len(labels)\n",
        "    class_weights = total_samples / (len(class_counts) * class_counts)\n",
        "\n",
        "    # weights as tensor (to pass to the loss function)\n",
        "    return torch.tensor(class_weights, dtype=torch.float)"
      ],
      "metadata": {
        "id": "DIM7fR7ktfzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConfusionMatrixCallback(Callback):\n",
        "    def __init__(self, class_names):\n",
        "        self.class_names = class_names\n",
        "        self.val_labels = []\n",
        "        self.val_preds = []\n",
        "\n",
        "    def on_validation_epoch_start(self, trainer, pl_module):\n",
        "        self.val_labels = []\n",
        "        self.val_preds = []\n",
        "\n",
        "    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx=0):\n",
        "        images, labels = batch\n",
        "        with torch.no_grad():\n",
        "            pl_module.eval()\n",
        "            images = images.to(pl_module.device)\n",
        "            outputs = pl_module(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            self.val_labels.extend(labels.cpu().numpy())\n",
        "            self.val_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "    def on_validation_epoch_end(self, trainer, pl_module):\n",
        "        cm = confusion_matrix(self.val_labels, self.val_preds)\n"
      ],
      "metadata": {
        "id": "4YD_bbBKtb9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, now we can analyze what has changed in this code:\n",
        "- The architecture is now defined by a configuration dictionary (architecture_config).\n",
        "- The convolutional blocks are created in a loop based on the configuration.\n",
        "- This allows us to easily modify the architecture by changing the configuration dictionary."
      ],
      "metadata": {
        "id": "gk-RKaL2sbTn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try running our network first"
      ],
      "metadata": {
        "id": "JSvQeq2EtuD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup data module\n",
        "batch_size = 64\n",
        "data_module = WasteDataModuleMulti(batch_size=batch_size)\n",
        "data_module.setup()\n",
        "\n",
        "# Compute class weights from training data\n",
        "class_weights = compute_class_weights(data_module.train_dataset)\n",
        "print(\"Class weights:\", class_weights)\n",
        "\n",
        "# Set class weights for weighted sampling\n",
        "data_module.class_weights = class_weights\n",
        "data_module.setup()  # Re-setup to compute sample weights"
      ],
      "metadata": {
        "id": "APPo2BCptsIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model with default architecture configuration\n",
        "# model = ParametricCNN()\n",
        "\n",
        "\n",
        "# Initialize model with custom architecture configuration\n",
        "architecture_config = {\n",
        "    'blocks': [\n",
        "        {'filters': 64, 'convs': 2},\n",
        "        {'filters': 128, 'convs': 2},\n",
        "        {'filters': 256, 'convs': 2},\n",
        "    ],\n",
        "    'dropout_conv': 0.25,\n",
        "    'fc_units': 512,\n",
        "    'dropout_fc': 0.5\n",
        "}\n",
        "\n",
        "model = ParametricCNN(architecture_config=architecture_config)"
      ],
      "metadata": {
        "id": "zvF9MACXt3hM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print model summary\n",
        "summary(model, (3, 224, 224), device=\"cpu\")\n"
      ],
      "metadata": {
        "id": "JZGlCSrdt73k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or we can make it different"
      ],
      "metadata": {
        "id": "tjJhXXqPt-Up"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "architecture_config_alt_alternative = {\n",
        "    'blocks': [\n",
        "        {'filters': 64, 'convs': 3},\n",
        "        {'filters': 128, 'convs': 3},\n",
        "        {'filters': 256, 'convs': 3},\n",
        "        {'filters': 512, 'convs': 2},\n",
        "    ],\n",
        "    'dropout_conv': 0.3,\n",
        "    'fc_units': 1024,\n",
        "    'dropout_fc': 0.5\n",
        "}\n",
        "\n",
        "model = ParametricCNN(architecture_config=architecture_config_alt_alternative)"
      ],
      "metadata": {
        "id": "85I-cb65uA-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print model summary\n",
        "summary(model, (3, 224, 224), device=\"cpu\")\n"
      ],
      "metadata": {
        "id": "ItPm4-zcuD6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK, this one \"looks fine\", let's try it"
      ],
      "metadata": {
        "id": "ugD9PrvDuSkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup callbacks\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath='checkpoints/lightning_multiclass',\n",
        "    filename='best',\n",
        "    save_top_k=1,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_last=True\n",
        ")\n",
        "\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=7,\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "# Confusion matrix callback\n",
        "class_names = list(final_classes.keys())\n",
        "confusion_matrix_callback = ConfusionMatrixCallback(class_names)"
      ],
      "metadata": {
        "id": "UQch8-HsuWZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup trainer\n",
        "accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
        "devices = list(range(torch.cuda.device_count())) if accelerator == 'gpu' else 1\n",
        "\n",
        "# low number, just to check everything works\n",
        "NUM_EPOCHS = 1\n",
        "\n",
        "trainer = Trainer(\n",
        "    max_epochs=NUM_EPOCHS,\n",
        "    callbacks=[checkpoint_callback, early_stopping_callback, confusion_matrix_callback],\n",
        "    accelerator=accelerator,\n",
        "    devices=devices\n",
        ")\n"
      ],
      "metadata": {
        "id": "GqWKjeGeudut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "trainer.fit(model, datamodule=data_module)"
      ],
      "metadata": {
        "id": "95d25wGVukF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "trainer.test(model, datamodule=data_module)"
      ],
      "metadata": {
        "id": "u84S0gzaul04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now try to experiment with different versions of this architecture.\n",
        "Right now we do not have any intuition about how the network should work, what it should have (elements, layers, blocks etc). We can only experiment to gain knowledge and understanding.\n",
        "\n",
        "The task is always the same - to create the best possible architecture. That means:\n",
        "\n",
        "\n",
        "*   Fastest possible\n",
        "*   Smallest size (memory footprint)\n",
        "*   Highest/lowest metric (usually accuracy)\n",
        "\n",
        "By changing our architecture we can try to reach balance between these points\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aSPNzb1wseM2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5H-LrgItwG3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASK: Experiment manually with different configurations of the network."
      ],
      "metadata": {
        "id": "NsCnHMdTso5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASK: Based on your knowledge implement a second architecture. This one will not be VGG-inspired, but based on AlexNET\n",
        "\n",
        "Implement modular versions of AlexNet based on this article:\n",
        "https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\n",
        "\n",
        "WARNING: The original AlexNet worked on dual GPUs, so you might need to adjust the architecture.\n",
        "\n",
        "If you want (suggestion, not necessity) you can read some more details here:\n",
        "https://arxiv.org/abs/1404.5997\n",
        "\n",
        "**Experiment with different configurations**\n",
        "\n",
        "**Evaluate performance on the dataset**\n"
      ],
      "metadata": {
        "id": "EH8g4dAvvXB7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YzP91V1FsIgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manual vs automatic tuning\n",
        "\n",
        "After experimenting with architectures we know one thing for sure - this process can be tedious. Making small changes to architecture (like number of layers, filters etc) will impact how the network behaves. However, this is a slow and boring process, that needs supervision. Fortunately there are always better ways to do this.\n",
        "\n",
        "Therefore let us try to automate it.\n",
        "We can even use what we already have installed and tested in previous classes - CometML framework."
      ],
      "metadata": {
        "id": "LM0hck-VwOkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "from pytorch_lightning import LightningModule, Trainer, LightningDataModule\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
        "from pytorch_lightning.loggers import CometLogger\n",
        "from torchsummary import summary\n",
        "import pytorch_lightning as pl\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import os\n",
        "import comet_ml\n",
        "\n",
        "# Set seed for reproducibility\n",
        "pl.seed_everything(42)"
      ],
      "metadata": {
        "id": "EcBqMduMw5yS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Data augmentations for training\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # ImageNet normalization\n",
        "])\n",
        "\n",
        "# Validation and test transforms\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "class WasteDataModuleMulti(LightningDataModule):\n",
        "    def __init__(self, root_dir='datasets/combined_waste_dataset',\n",
        "                 batch_size=32):\n",
        "        super().__init__()\n",
        "        self.root_dir = root_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.num_classes = 9  # organic, battery, glass, metal, paper, cardboard, plastic, textiles, trash\n",
        "        self.class_weights = None\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.train_dataset = CombinedWasteDatasetMulti(\n",
        "            root_dir=self.root_dir, split='train', transform=train_transform\n",
        "        )\n",
        "        self.val_dataset = CombinedWasteDatasetMulti(\n",
        "            root_dir=self.root_dir, split='val', transform=val_test_transform\n",
        "        )\n",
        "        self.test_dataset = CombinedWasteDatasetMulti(\n",
        "            root_dir=self.root_dir, split='test', transform=val_test_transform\n",
        "        )\n",
        "\n",
        "        self.num_workers = os.cpu_count() - 1\n",
        "\n",
        "\n",
        "        if self.class_weights is not None:\n",
        "            self.sample_weights = [self.class_weights[label].item() for _, label in self.train_dataset.data]\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        if hasattr(self, 'sample_weights'):\n",
        "\n",
        "            sampler = WeightedRandomSampler(self.sample_weights, len(self.train_dataset), replacement=True)\n",
        "\n",
        "            return DataLoader(self.train_dataset, batch_size=self.batch_size,\n",
        "                              sampler=sampler, num_workers=self.num_workers)\n",
        "        else:\n",
        "            return DataLoader(self.train_dataset, batch_size=self.batch_size,\n",
        "                              shuffle=True, num_workers=self.num_workers)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset, batch_size=self.batch_size,\n",
        "                          shuffle=False, num_workers=self.num_workers)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_dataset, batch_size=self.batch_size,\n",
        "                          shuffle=False, num_workers=self.num_workers)"
      ],
      "metadata": {
        "id": "mo50MARMw9Se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ParametricCNN(LightningModule):\n",
        "    def __init__(self, num_classes=9, class_weights=None, architecture_config=None, learning_rate=1e-3):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.num_classes = num_classes\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Default architecture config (our first, \"VGG-like\")\n",
        "        if architecture_config is None:\n",
        "            architecture_config = {\n",
        "                'blocks': [\n",
        "                    {'filters': 64, 'convs': 2},\n",
        "                    {'filters': 128, 'convs': 2},\n",
        "                    {'filters': 256, 'convs': 2},\n",
        "                ],\n",
        "                'dropout_conv': 0.25,\n",
        "                'fc_units': 512,\n",
        "                'dropout_fc': 0.5\n",
        "            }\n",
        "\n",
        "        self.architecture_config = architecture_config\n",
        "\n",
        "        # Build convolutional layers\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for block in architecture_config['blocks']:\n",
        "            for _ in range(block['convs']):\n",
        "                layers.append(nn.Conv2d(in_channels, block['filters'], kernel_size=3, padding=1))\n",
        "                layers.append(nn.BatchNorm2d(block['filters']))\n",
        "                layers.append(nn.ReLU())\n",
        "                in_channels = block['filters']\n",
        "            layers.append(nn.MaxPool2d(2, 2))\n",
        "            layers.append(nn.Dropout(architecture_config['dropout_conv']))\n",
        "\n",
        "        layers.append(nn.Flatten())\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            dummy_input = torch.zeros(1, 3, 224, 224)\n",
        "            output = torch.tensor(dummy_input) # so we will have a tensor to save results into\n",
        "\n",
        "            for layer in layers[:-1]:\n",
        "                output = layer(output)\n",
        "            self.flatten_size = output.numel() # numel = number of elements in the tensor\n",
        "\n",
        "        # Add fully connected layers\n",
        "        layers.append(nn.Linear(self.flatten_size, architecture_config['fc_units']))\n",
        "        layers.append(nn.BatchNorm1d(architecture_config['fc_units']))\n",
        "        layers.append(nn.ReLU())\n",
        "        layers.append(nn.Dropout(architecture_config['dropout_fc']))\n",
        "        layers.append(nn.Linear(architecture_config['fc_units'], num_classes))\n",
        "\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "        # Loss function\n",
        "\n",
        "        self.loss_fn = nn.CrossEntropyLoss(weight=class_weights) if class_weights is not None else nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        images, labels = batch\n",
        "        outputs = self(images)\n",
        "        loss = self.loss_fn(outputs, labels)\n",
        "        self.log('train_loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        images, labels = batch\n",
        "        outputs = self(images)\n",
        "        loss = self.loss_fn(outputs, labels)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        acc = (predicted == labels).float().mean()\n",
        "        self.log('val_loss', loss)\n",
        "        self.log('val_acc', acc)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        images, labels = batch\n",
        "        outputs = self(images)\n",
        "        loss = self.loss_fn(outputs, labels)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        acc = (predicted == labels).float().mean()\n",
        "        self.log('test_loss', loss)\n",
        "        self.log('test_acc', acc)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.1)\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": {\n",
        "                \"scheduler\": scheduler,\n",
        "                \"monitor\": \"val_loss\",\n",
        "            },\n",
        "        }\n"
      ],
      "metadata": {
        "id": "ncuCctCNw_5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute class weights for imbalanced dataset\n",
        "def compute_class_weights(dataset):\n",
        "    labels = [label for _, label in dataset.data]\n",
        "    class_counts = np.bincount(labels, minlength=9)\n",
        "    total_samples = len(labels)\n",
        "    class_weights = total_samples / (len(class_counts) * class_counts)\n",
        "\n",
        "    # weights as tensor (to pass to the loss function)\n",
        "    return torch.tensor(class_weights, dtype=torch.float)\n",
        "\n",
        "\n",
        "class ConfusionMatrixCallback(Callback):\n",
        "    def __init__(self, class_names):\n",
        "        self.class_names = class_names\n",
        "        self.val_labels = []\n",
        "        self.val_preds = []\n",
        "\n",
        "    def on_validation_epoch_start(self, trainer, pl_module):\n",
        "        self.val_labels = []\n",
        "        self.val_preds = []\n",
        "\n",
        "    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx=0):\n",
        "        images, labels = batch\n",
        "        with torch.no_grad():\n",
        "            pl_module.eval()\n",
        "            images = images.to(pl_module.device)\n",
        "            outputs = pl_module(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            self.val_labels.extend(labels.cpu().numpy())\n",
        "            self.val_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "    def on_validation_epoch_end(self, trainer, pl_module):\n",
        "        cm = confusion_matrix(self.val_labels, self.val_preds)\n",
        "\n"
      ],
      "metadata": {
        "id": "gCbqOtVPxCuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup data module\n",
        "batch_size = 64\n",
        "data_module = WasteDataModuleMulti(batch_size=batch_size)\n",
        "data_module.setup()\n",
        "\n",
        "# Compute class weights from training data\n",
        "class_weights = compute_class_weights(data_module.train_dataset)\n",
        "print(\"Class weights:\", class_weights)\n",
        "\n",
        "# Set class weights for weighted sampling\n",
        "data_module.class_weights = class_weights\n",
        "data_module.setup()  # Re-setup to compute sample weights\n"
      ],
      "metadata": {
        "id": "SEgrkQYPxFcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning Setup with CometML\n",
        "\n",
        "We use CometML's Optimizer for automated hyperparameter search.\n",
        "\n",
        "This allows us to search over architecture configurations efficiently.\n",
        "\n",
        "More details are available here (for example):\n",
        "\n",
        "https://www.comet.com/site/blog/hyperparameter-optimization-with-comet/\n"
      ],
      "metadata": {
        "id": "qy7jpuQoxGsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for testing purpuses let's set this to a very low value\n",
        "NUM_EPOCHS = 1\n"
      ],
      "metadata": {
        "id": "6_CB9kokxT7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create and train a model with given hyperparameters\n",
        "def train_model_with_params(params, data_module, class_weights, num_epochs=NUM_EPOCHS):\n",
        "    # Create architecture config from params\n",
        "    architecture_config = {\n",
        "        'blocks': [\n",
        "            {'filters': params['filters_block1'], 'convs': params['convs_block1']},\n",
        "            {'filters': params['filters_block2'], 'convs': params['convs_block2']},\n",
        "            {'filters': params['filters_block3'], 'convs': params['convs_block3']},\n",
        "        ],\n",
        "        'dropout_conv': params['dropout_conv'],\n",
        "        'fc_units': params['fc_units'],\n",
        "        'dropout_fc': params['dropout_fc']\n",
        "    }\n",
        "\n",
        "    # Initialize model\n",
        "    model = ParametricCNN(architecture_config=architecture_config, class_weights=class_weights, learning_rate=params['learning_rate'])\n",
        "\n",
        "    # Comet Logger for this experiment\n",
        "    comet_logger = CometLogger(\n",
        "            project=\"waste-classification-multiclass\",\n",
        "            name=\"lab4-hyperparam-tuning\")\n",
        "\n",
        "    # Callbacks\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        dirpath='checkpoints/lightning_multiclass',\n",
        "        filename='best-{epoch:02d}-{val_loss:.2f}',\n",
        "        save_top_k=1,\n",
        "        monitor='val_loss',\n",
        "        mode='min',\n",
        "        save_last=True\n",
        "    )\n",
        "\n",
        "    early_stopping_callback = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=7,\n",
        "        mode='min'\n",
        "    )\n",
        "\n",
        "    confusion_matrix_callback = ConfusionMatrixCallback(list(final_classes.keys()))\n",
        "\n",
        "    # Trainer\n",
        "    accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
        "    devices = [0]  # using a single GPU for hyperparameter tuning to avoid complications\n",
        "                   # TODO: IF you have access to multiple GPUs, you can try and see what happens, with the CURRENT code configuration\n",
        "                   # and distribution strategy set in Pytorch Lightning Trainer.\n",
        "\n",
        "    trainer = Trainer(\n",
        "        max_epochs=num_epochs,\n",
        "        callbacks=[checkpoint_callback, early_stopping_callback, confusion_matrix_callback],\n",
        "        accelerator=accelerator,\n",
        "        devices=devices,\n",
        "        logger=comet_logger\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    trainer.fit(model, datamodule=data_module)\n",
        "    # this will log \"train_loss\", \"val_loss\" and \"val_acc\"  metrics\n",
        "    return trainer.callback_metrics['val_loss'].item()\n",
        "\n",
        "\n",
        "    # This part is commented out for a reason - more on that below\n",
        "    # # Test the model\n",
        "    # trainer.test(model, datamodule=data_module)\n",
        "    # # this will log \"test_loss\" and \"test_acc\" logged metrics\n",
        "\n",
        "    # # Return the best validation loss for optimization\n",
        "    # return trainer.callback_metrics['test_loss'].item()\n"
      ],
      "metadata": {
        "id": "CfpoiI-BxZbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now must as ourselves a very important questions - which is correct here? Can we use validation loss or test loss for hyperparameter tuning?\n",
        "Or, to be more precise, are we ALLOWED to use test loss for hyperparameter tuning?\n",
        "\n",
        "**TODO: Think about this question carefully and answer yourselves**\n",
        "\n",
        "I will give you my point of view on this question:\n",
        "\n",
        "In general, you should NOT use test loss for hyperparameter tuning.\n",
        "The test set is supposed to be a completely unseen dataset that you only use at the very end of your model development process to get an unbiased estimate of your model's performance.\n",
        "\n",
        "If you use the test set for hyperparameter tuning, you are effectively \"peeking\" at the test data, using the test set to guide your model selection.\n",
        "\n",
        "This can lead to overfitting to the test set and an overly optimistic estimate of your model's performance, not reflective of its true generalization ability.\n",
        "\n",
        "Therefore, it is best practice to use a separate validation set for hyperparameter tuning, and only use the test set once you have finalized your model.\n",
        "\n",
        "If you, however, have a different oppinion on this matter, I would be very interested to hear it. Never to argue/mock, but to discuss what it really means to have a \"good\" model evaluation strategy, as this is one of the most important parts of creating and later deploting a proper model.\n"
      ],
      "metadata": {
        "id": "RRCXHvaPxhvD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using CometML Optimizer for Hyperparameter Tuning\n",
        "Main references:\n",
        "\n",
        "**TODO: READ THESE LINKS THOROUGHLY**\n",
        "\n",
        "using Comet Optimizer is an advanced topic, and to use it properly it is ALWAYS advised to read the documentation.\n",
        "\n",
        "Truth be told - even with the documentation it can be tricky to set up properly.\n",
        "\n",
        "But once set up, it can save you a lot of time and effort in hyperparameter tuning.\n",
        "\n",
        "Digresion: Any hyperparameter tuning framework is \"complicated\" to set up properly.\n",
        "Because hyperparameter tuning is inherently complicated.\n",
        "\n",
        "You need to make sure that your training code is properly modularized,\n",
        "that hyperparameters are passed correctly, that logging is set up properly, etc.\n",
        "\n",
        "So don't be discouraged if it takes some time to get it right.\n",
        "Once you have a working setup, you can reuse it for future projects.\n",
        "\n",
        "\n",
        "https://www.comet.com/docs/v2/api-and-sdk/python-sdk/reference/Optimizer/\n",
        "\n",
        "https://www.comet.com/docs/v2/guides/optimizer/quickstart/\n"
      ],
      "metadata": {
        "id": "oOUYorMEyE9V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Source: https://www.comet.com/docs/v2/guides/optimizer/quickstart/*\n",
        "\n",
        "Before you begin optimization, you need to choose:\n",
        "\n",
        "    * The hyperparameters to tune.\n",
        "    * The search space for each hyperparameter to tune.\n",
        "    * The search algorithm (one of grid search, random search, and Bayesian optimization).\n",
        "\n",
        "Additionally, make sure to refactor your existing model training code so that hyperparameters are defined as parametrized variables."
      ],
      "metadata": {
        "id": "dVnRjxyGyHiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define hyperparameter search space\n",
        "# CometML Optimizer uses this to suggest parameters\n",
        "\n",
        "# SOOOOOOOOO MANY parameters to tune here...\n",
        "# is it too much maybe? Will we even be able to train anything in reasonable time?\n",
        "\n",
        "param_space = {\n",
        "    \"filters_block1\": {\"type\": \"discrete\", \"values\": [32, 64, 128]}, # DO NOT USE INTEGER, where there is so much difference between values\n",
        "    \"convs_block1\": {\"type\": \"integer\", \"min\": 1, \"max\": 3}, # here is ok\n",
        "    \"filters_block2\": {\"type\": \"discrete\", \"values\": [64, 128, 256]},\n",
        "    \"convs_block2\": {\"type\": \"integer\", \"min\": 1, \"max\": 3},\n",
        "    \"filters_block3\": {\"type\": \"discrete\", \"values\": [128, 256, 512]},\n",
        "    \"convs_block3\": {\"type\": \"integer\", \"min\": 1, \"max\": 3},\n",
        "    \"dropout_conv\": {\"type\": \"float\", \"min\": 0.1, \"max\": 0.5},\n",
        "    \"fc_units\": {\"type\": \"discrete\", \"values\": [256, 512, 1024]},\n",
        "    \"dropout_fc\": {\"type\": \"float\", \"min\": 0.3, \"max\": 0.7}, # TODO: WHY NOT DISCRETE? or maybe it should be discrete????????\n",
        "    \"learning_rate\": {\"type\": \"float\", \"scalingType\": \"loguniform\", \"min\": 1e-4, \"max\": 1e-2} # TODO: log uniform?? what is that??? Try to read about this parameter\n",
        "                                                                                              # other options are: ['linear', 'uniform', 'loguniform', 'normal', 'lognormal']\n",
        "}\n"
      ],
      "metadata": {
        "id": "IomNoBNayktG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: look at the parameters above.\n",
        "\n",
        "Do we really need to test all of them, especially at once? If not, maybe you can trim this parameters space?\n"
      ],
      "metadata": {
        "id": "vFSNXCeJypIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Strategy 1: Random Search (Active)\n",
        "\n",
        "Random search samples parameters randomly from the space.\n",
        "\n",
        "It's simple, effective, and doesn't assume parameter relationships.\n",
        "\n",
        "Random search is, as the name implies, randomly selecting combinations of hyperparameter values to try out.\n",
        "\n",
        "It can be good, if we do not know much about the hyperparameter space,\n",
        "or if we suspect that only a few hyperparameters have a significant impact on performance.\n",
        "\n",
        "As with a lot of things in machine learning, doing something randomly can sometimes outperform more \"intelligent\" approaches.\n",
        "\n",
        "Though Grid is still usually better than pure random guessing."
      ],
      "metadata": {
        "id": "qFGZKDuzy7O-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Source: https://www.comet.com/site/blog/hyperparameter-optimization-with-comet/*\n",
        "\n",
        "Random search offers slightly more flexibility than grid search.\n",
        "Instead of exhaustively iterating through all possible combinations like in the grid search algorithm,\n",
        "random search selects combinations at random from the possible parameter\n",
        "values until the run is explicitly stopped or the max combinations are met.\n",
        "\n",
        "Similar to grid search, the random algorithm does not use past experiments to inform future experiments,\n",
        "but when only a small number of hyperparameters have an effect on the final model performance,\n",
        "the random search can outperform grid search."
      ],
      "metadata": {
        "id": "HEnNqGJYzE7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.comet.com/docs/v2/guides/optimizer/configure-optimizer/\n",
        "config_dict = {\n",
        "    \"algorithm\": \"random\",\n",
        "    \"spec\": {\n",
        "        \"metric\": \"accuracy\",\n",
        "        \"maxCombo\": 5, # VERY important parameter - make sure you know what it does\n",
        "        \"gridSize\": 5,\n",
        "        \"minSampleSize\": 150,\n",
        "    },\n",
        "    \"parameters\": param_space,\n",
        "    \"name\": \"My Random Search\",\n",
        "    \"trials\": 1, # read the docs to understand what this does - might be misleading at first\n",
        "}"
      ],
      "metadata": {
        "id": "wDNf8EPCzKm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt_random = comet_ml.Optimizer(config=config_dict)"
      ],
      "metadata": {
        "id": "mvVWIBSCzO27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is our actual hyperparameter tuning loop\n",
        "As you can see we are calling our training function with different parameters suggested by the optimizer\n",
        "This makes it very easy to integrate hyperparameter tuning into existing training code\n",
        "\n",
        "This is mostly the same thing as calling it manually, but we do not have to implement the tuning \"strategy\" ourselves\n",
        "or, properly called, the \"search algorithm\".\n",
        "This is the main point of all the frameworks for hyperparameter tuning - to avoid reinventing the wheel\n",
        "But you could easily do this yourself if you wanted to.\n",
        "\n",
        "\n",
        "What we are doing is called \"black-box optimization\", because we do not have to know anything about the function we are optimizing\n",
        "we just provide inputs (hyperparameters) and get outputs (validation loss)"
      ],
      "metadata": {
        "id": "uBc14SMAzTmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the optimization\n",
        "for experiment in opt_random.get_experiments():\n",
        "    params = experiment.params\n",
        "    val_loss = train_model_with_params(params, data_module, class_weights, num_epochs=NUM_EPOCHS)  # Short epochs for demo\n",
        "    experiment.log_metric(\"val_loss\", val_loss)"
      ],
      "metadata": {
        "id": "bb1xA2JQzWwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Strategy 2: Bayesian Optimization (Active Alternative)\n",
        "Bayesian optimization uses probabilistic models to suggest promising parameters.\n",
        "\n",
        "It's more efficient than random search for continuous spaces.\n",
        "It is an \"intelligent\" search strategy that builds a model of the hyperparameter space and uses it to select the most promising hyperparameters to evaluate next.\n",
        "\n",
        "This approach can be more efficient than random search, especially when the hyperparameter space is large and continuous, as it focuses on areas likely to yield better performance.\n"
      ],
      "metadata": {
        "id": "Cmc3pogozdQD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Source: https://www.comet.com/site/blog/hyperparameter-optimization-with-comet/*\n",
        "\n",
        "Comet documentation states the Bayes algorithm may be the best choice for most of your Optimizer uses.\n",
        "\n",
        "    Bayesian optimization has been shown to obtain better results in fewer evaluations compared to grid search and random search,\n",
        "    due to the ability to reason about the quality of experiments before they are run.  Wikipedia\n",
        "\n",
        "Bayes optimization works by iteratively evaluating a promising hyperparameter configuration\n",
        "based on the current model, then updating it. The main aim of the technique is to gather observations\n",
        "that reveal as much information as possible about the location of the optimum."
      ],
      "metadata": {
        "id": "2xzbGHErzm5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config_dict = {\n",
        "    \"algorithm\": \"bayes\",\n",
        "    \"spec\": {\n",
        "        \"objective\": \"maximize\",\n",
        "        \"metric\": \"accuracy\",\n",
        "        \"maxCombo\": 5, # VERY important parameter - make sure you know what it does\n",
        "        \"retryLimit\": 10,\n",
        "        \"retryAssignLimit\": 10,\n",
        "    },\n",
        "    \"parameters\": param_space,\n",
        "    \"name\": \"My Bayesian Search\",\n",
        "    \"trials\": 1,\n",
        "}\n",
        "\n",
        "opt_bayes = comet_ml.Optimizer(config=config_dict)"
      ],
      "metadata": {
        "id": "IJZOjQ2NzqM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the optimization\n",
        "for experiment in opt_bayes.get_experiments():\n",
        "    params = experiment.params\n",
        "    val_loss = train_model_with_params(params, data_module, class_weights, num_epochs=NUM_EPOCHS)\n",
        "    experiment.log_metric(\"val_loss\", val_loss)"
      ],
      "metadata": {
        "id": "8z6AVQuOzsh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other Strategies Available in CometML:\n",
        "- Grid Search: Exhaustive search over specified combinations.\n",
        "  Set algorithm=\"grid\" and provide a list of values for each parameter.\n",
        "\n",
        "**TODO: Implement grid search yourself**\n",
        "\n",
        "It is more widely used than random search, but not as much as Bayesian optimization.\n",
        "Pretty please read the documentation to understand how to set up the parameters dictionary for grid search.\n",
        "Not all the parameters exist for every search strategy!\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GP9lsigPzvzw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**TODO: Is this the only way to do hyperparameter tuning with PyTorch Lightning?**\n",
        "\n",
        "Of course not. There are many other libraries and frameworks available for hyperparameter tuning, such as Optuna, Ray Tune, etc. Each has its own strengths and weaknesses.\n",
        "\n",
        "Even PyTorch Lightning itself has built-in support for very simple tuning:\n",
        "\n",
        "https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.tuner.tuning.Tuner.html#lightning.pytorch.tuner.tuning.Tuner\n",
        "\n",
        "CometML is just one option that integrates well with PyTorch Lightning and provides a user-friendly interface for tracking experiments and visualizing results.\n",
        "Depending on your specific needs and preferences, you may find other tools more suitable.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6cho_ySpz1bs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Homework Task:\n",
        "based on what you have learned in this lab, implement hyperparameter tuning using Optuna\n",
        "\n",
        "You will find more information about Optuna here:\n",
        "\n",
        "https://optuna.org/\n",
        "\n",
        "\n",
        "and here:\n",
        "\n",
        "https://pytorch-lightning.readthedocs.io/en/stable/extensions/optuna.html\n",
        "\n",
        "\n",
        "and also here:\n",
        "\n",
        "https://optuna-integration.readthedocs.io/en/stable/index.html"
      ],
      "metadata": {
        "id": "gDaO-Mi5z_yP"
      }
    }
  ]
}